---
bibliography: references.bib
csl: apa.csl
format: 
  docx:
    reference-doc: reference-doc.docx
output:
  officedown::rdocx_document:
    page_margins:
      bottom: 1
      footer: 0
      gutter: 0
      header: 0.5
      left: 1
      right: 1
      top: 1
    plots:
      align: center
      caption:
        pre: 'Figure '
        sep: '. '
        style: Image Caption
    tables:
      caption:
        pre: 'Table '
        sep: '. '
        style: Table Caption
  pdf_document: default
  word_document: default
editor: 
  markdown: 
    wrap: sentence
---

```{r include = FALSE}
library(flextable)
library(stringr)
library(dplyr)
library(english)

source("../scripts/custom_functions/general-functions.R")
load("staged_results.RData")


knitr::opts_chunk$set(
  echo = F,
  fig.align = "center",
  fig.pos = "!t", 
  out.extra = "",
  fig.show = "asis",
  message = FALSE,
  tab.topcaption = T,
  warning = FALSE
)

# set up flextable for tables
set_flextable_defaults(
  font.family = "Times", 
  font.size = 10,
  font.color = "black",
  line_spacing = 1,
  padding.bottom = 1, 
  padding.top = 1,
  padding.left = 1,
  padding.right = 1
)

```

# Proposal Research Highlights

1. We use Drift Diffusion Modeling (DDM) to investigate how two forms of early-life adversity---material deprivation and household threat---are associated with lowered or improved cognitive performance.
2. The DDM can inform us if and where cognitive differences occur along distinct stages of cognitive processing.
3. We will also use structural equation modeling and out-of-sample validation to tease apart effects of adversity that are task-specific versus task-general.
4. We will apply our approach to a large, representative sample of around 10,000 9- to 10 year-olds from the ABCD study.

\pagebreak

# Proposal Abstract

Childhood adversity can lead to cognitive deficits or enhancements, depending on many factors. 
Though progress has been made, two challenges prevent us from integrating and better understanding these findings. 
First, studies commonly use and interpret raw performance differences, such as mean response times or overall accuracy. 
However, raw scores conflate different stages of cognitive processing. 
Second, studies tend to either isolate or aggregate abilities, obscuring the degree to which individual differences reflect task-specific or task-general processes. 
We address these challenges using Drift Diffusion Modeling (DDM) and structural equation modeling.
Combining these techniques, we can (1) relate early-life adverse experiences to individual differences in different stages of processing, and (2) investigate whether these reflect differences in specific or task-general performance. 
We examine how two forms of adversity---material deprivation and household threat---affect performance on four cognitive tasks in a large, representative sample of 9-10 year-olds from the Adolescent Brain Cognitive Development (ABCD) study. 
This approach holds promise for both deficit- and strength-based research questions. 
It will add much-needed nuance to adversity-related performance differences, which can inform theory and intervention.

*Keywords:* adversity, cognitive deficits, cognitive enhancements, drift diffusion modeling, Adolescent Brain Cognitive Development (ABCD) Study 

\pagebreak

#### Cognitive deficits and enhancements in youth from adverse conditions: An integrative assessment using Drift Diffusion Modeling in the ABCD study

<br>

The effects of early-life adversity---such as growing up in poverty or experiencing high levels of violence---on cognition are complex.
There is a growing consensus that adversity-exposed youth may develop not only deficits, but also strengths.
For example, studies find lowered and improved performance across different cognitive domains including (but not limited to) executive functioning, social cognition, language, and emotion [@ellis_2022; @frankenhuis_2013; @frankenhuis_2016;  @sheridan_2022; @sheridan_2014].
Researchers focused on one type of effect or another acknowledge the importance of identifying both deficits and strengths.
Yet, in practice, they often focus on one at the expense of the other.
To develop an integrated, well-rounded, and nuanced understanding of how adversity shapes cognitive abilities, research must integrate both types of effects.

Such an integration of deficit- and strength-based approaches is hampered by two methodological challenges.
First, most cognitive tasks involve different stages of processing which are obscured when analyzing raw performance differences.
This makes it difficult to understand why cognitive performance may be lowered or improved. 
Second, adversity may lower or improve performance because it affects general processes (i.e., processes shared across many tasks) or abilities that are task-specific.
In this Registered Report, we use a framework that tackles both challenges.
First, we decompose raw performance into measures of different stages of cognitive processes through cognitive modeling.
Second, we analyze four different tasks---tapping processing speed, attention shifting, inhibition, and mental rotation---all of which have documented associations with adversity.
Finally, we model shared (i.e., task-general) and unique (i.e., task-specific) factors that drive performance and investigate how they are associated with adversity.

## What do deficit and enhancement patterns mean? {#intro_sub1}

Both the deficit and strength-based literature often use speeded tasks, in which participants are usually instructed to respond as fast and accurate as possible.
For example, performing well on inhibition tasks [e.g., Flanker task, Go/No-Go Task\; @farah_2006; @fields_2021; @mezzacappa_2004; @noble_2005], attention shifting tasks [e.g., Dimensional Change Card Sort\; @farah_2006; @fields_2021; @nweze_2021; @young_2022; @mittal_2015; @noble_2005], and stimulus detection tasks [@farah_2006; @noble_2005; @pollak_2008] requires fast and accurate responses.
In practice, performance is often quantified using aggregated indices of speed alone (e.g., RT), accuracy alone (e.g., proportion correct), or both independently (rather than in an integrated manner).

In both the deficit and strength-based literature, *task performance* (indexed by mean RTs or accuracy) is routinely equated with *cognitive ability*.
For example, deficit-focused studies relate slower RTs on inhibition tasks to *worse inhibition ability* [@farah_2006; @fields_2021; @mezzacappa_2004; @noble_2005].
Strength-based studies relate faster RTs on standard attention shifting tasks to *better shifting ability* [@fields_2021; @young_2022; @mittal_2015].
However, speed and accuracy comprise more than pure ability (e.g., inhibition, attention shifting).
They also measure other constructs such as response caution (e.g., more or less cautious responding), speed of task preparation (e.g., orienting attention, encoding information), and speed of response execution. 
This heterogeneity creates an inferential risk, namely, if performance differences are interpreted as differences in abilities without sufficiently considering alternative explanations.
In addition, the effect of adversity exposure may not be limited to a single process.
For example, a specific type of adversity could affect both the speed of information processing and also shape the strategy that a person uses.
These inferential challenges have real-world implications, especially when raw performance is used as an early screening tool to assess cognitive abilities [@distefano_2021].

One promising solution to these issues is leveraging cognitive measurement models developed by mathematical psychologists.
For speeded binary decision tasks, a well-established measurement model is the Drift Diffusion Model [DDM\; @forstmann_2016; @ratcliff_1998; @ratcliff_2008; @wagenmakers_2009].
The DDM integrates speed and accuracy on a trial-by-trial level to estimate cognitive processes at different stages of the decision-making process.
The DDM assumes that people go through three distinct phases on each trial (see Figure 1 for a visualization).
The first phase, *preparation*, includes processes such as focusing attention and visually encoding the stimulus.
In the second phase, *decision-making*, people gather evidence for both response options until the evidence sufficiently favors one option over the other (explained below) and the decision process terminates. 
The third phase, *execution*, involves preparing and executing the motor response corresponding to the choice.

<br>
<br>
[Figure 1 about here]
<br>
<br>

DDM estimates a set of parameters^1^ for each participant that represent each phase of the decision process [@voss_2004]. 
The *drift rate* (*v*) represents the speed of information uptake [@schmiedek_2007; @voss_2013]. 
People with a higher drift rate are faster and make fewer errors. 
The *non-decision time* (*t0*) includes initial preparatory processes (e.g., visually encoding the stimulus) and processes after the decision is made (e.g., pressing a button). 
All else being equal, longer non-decision times reflect slower information processing but without a cost nor benefit in accuracy.
The *boundary separation* (*a*) represents the distance between the two decision boundaries.
A larger boundary separation means more information is collected before making a decision.
Thus, boundary separation measures response caution.
In contrast to non-decision time, larger boundary separation leads to slower but more accurate responses, reflecting a speed-accuracy tradeoff. 

As mentioned earlier, adversity-related raw performance differences---both lowered and improved performance---are typically interpreted as differences in ability (e.g., inhibition, attention shifting).
If these interpretations are accurate, then drift rate would reflect these variations.
That is because improved ability would result in both decreased RTs and increased accuracy.
However, if performance differences arise through other factors---such as differences in response caution or response speed---they would be captured by parameters other than the drift rate.
Thus, disentangling the drift rate, non-decision time, and boundary separation enhances our understanding of how adversity-exposure is associated with performance.

## Are deficit and enhancement patterns task-specific or task-general? {#intro_sub2}

An important caveat to interpreting task performance on any task in isolation is that performance on most tasks relies both on shared cognitive processes and unique abilities.
For example, RTs on executive functioning tasks are substantially confounded with general processing efficiency [@frischkorn_2019; @lerche_2020; @loffler_2022]. 
Both task-specific abilities and task-general processes affect RTs and accuracy in similar ways and are thus likely confounded in drift rates.
Task-general effects create the illusion that many different abilities are affected by adversity when in fact only one more general process is affected. 
Consider research on cognitive deficits. 
Adversity exposure might disrupt general cognitive processes shared across many tasks, such as general processing speed, for example, because of its effects on brain regions that are involved across several cognitive abilities [@sheridan_2014]. 
If so, studies analyzing raw Flanker performance in isolation will find processing speed deficits but wrongly interpret this as an inhibition deficit. 
Such distinctions matter for both deficit- and strength-based approaches (e.g., does adversity impair broad domains such as executive functioning? Does it enhance specific abilities such as attention shifting?), as well as for real-world interventions grounded in both approaches (e.g., school-based interventions targeting broad domains or specific abilities).

Structural equation modeling (SEM) can disentangle task-general and task-specific processes. 
For example, it can estimate shared task variance with latent task-general variables.
By estimating shared variance across different tasks, we can also obtain more precise estimates of task-specific abilities (i.e., variance unique to specific tasks).
@bignardi_2022 recently applied this approach to model how SES is related to standard performance measures in three large data sets.
They used SEM to model the effect of SES on a general factor and task-specific residual variances.
Lower SES was associated with a lower general ability, but *enhanced* task-specific processing speed, inhibition, and attention shifting.
However, their analysis looked at shared and unique variance using raw performance measures.
Thus, it is subject to the same limitations outlined in the previous section.

## The current study {#intro_current}

Here, we will analyze the Adolescent Brain Cognitive Development (ABCD) study data (http://abcdstudy.org). 
The ABCD study is ideal because it provides a large, representative, and socioeconomically and ethnically diverse sample of 9- to 10 year-olds--—an age range characterized by rapid growth in cognitive abilities [@blakemore_2006]. 

We will study two dimensions of adversity: household threat and material deprivation.
These forms of adversity have been widely studied in their relation to cognitive outcomes---from both deficit and strength-based perspectives [@fields_2021; @schafer_2022; @sheridan_2022; @young_2022]---and are central to contemporary conceptualizations of adversity [e.g., @mclaughlin_2021; @sheridan_2014]. 
Prior work has shown that cognitive deprivation is more strongly associated with lower cognitive performance than threat exposure [@salhi_2021; @sheridan_2020]. 
Although material deprivation (as measured here) and cognitive deprivation (in previous studies) are not identical, both seem related to access to resources that support cognitive development (e.g., books in the home, formal education). 
Indeed, in the ABCD sample material deprivation is highly or moderately correlated with income (-.81) and education (-.56), while correlations with household threat are lower [-.25 and -.12, respectively\; @dejoseph_2022]. 
Therefore, to the extent that the deprivation-versus-threat literature has captured ability-relevant processes, we may expect material deprivation to be more strongly associated with lower drift rates than threat exposure.

We will analyze four cognitive abilities that have been studied in relation to adversity. 
We include *attention shifting* because previous work has reported enhancement of this ability in children and (young) adults with more exposure to environmental unpredictability [based on raw performance switch costs\; @fields_2021; @mittal_2015; @young_2022; but see @nweze_2021]. 
Theoretically, attention shifting is thought to enable people to rapidly adjust to, and take advantage of, a changing environment (e.g., seize fleeting opportunities). 
We include *inhibition* because previous research suggests that children with more adverse experiences are worse at inhibiting distracting information [based on raw RT difference scores\; @fields_2021; @mezzacappa_2004; @mittal_2015; @tibu_2016].
We include *mental rotation* because previous studies have found negative associations between SES and mental rotation ability [based on RTs and accuracy\; @assari_2020; @bignardi_2022]. 
To the extent that these performance differences reflect differences in the respective abilities---as they have been interpreted---they should show up in *task-specific drift rates*.
We also include a measure of *processing speed*, which will not be measured in relation to adversity but provides a direct measure of the type of basic processing speed that plays a role in the other tasks. 
Taken together, the four tasks provide a broad assessment of cognitive domains, which makes them well-suited for isolating task-general processes. 
As all four tasks adhere to DDM assumptions, we can compare them based on the same model parameters.

Adaptation-based frameworks predict increased task-specific drift rates.
This follows from the key assumption that adversity shapes specific abilities, rather than general cognitive processes [@ellis_2022; @frankenhuis_2020; @frankenhuis_2016; @frankenhuis_2013].
Task-specific enhancement in the attention-shifting drift rate would align with this assumption, as this ability is thought to be adaptive in changing environments; but enhancement in the task-general drift rate would not.
One study reports evidence suggesting that exposure to threat but not deprivation is associated with better attention shifting [@young_2022]. 
If so, we should expect to see higher task-specific drift rates with household threat, but not with material deprivation.
Enhanced task-specific drift rates on inhibition and mental rotation would be unexpected yet interesting.
It would constitute novel documentation of enhancements, and would suggest that lowered raw performance reflects ability-irrelevant processes.
Finally, equivalent drift rates across adversity levels would also not be consistent with strength-based frameworks; rather, such a pattern would suggest that abilities are intact (i.e., not affected by adversity).

Deficit perspectives can accommodate both lowered task-specific and lowered task-general drift rates.
On the one hand, past work suggests that adversity impairs specific abilities [e.g., inhibition\; @farah_2006; @fields_2021; @mezzacappa_2004; @mittal_2015]. 
On the other hand, there is also evidence that adversity affects general cognitive ability [@bignardi_2022]---perhaps through its broad effects on brain regions that are involved across several cognitive abilities [@sheridan_2014]. 
However, equivalent or enhanced drift rates, whether they be task-specific or task-general, would not be consistent with deficit perspectives; rather, this would suggest that abilities are intact or enhanced.

Our approach adds value in a third way besides separating drift rate from ability-irrelevant factors and isolating task-specific and task-general effects: It allows us to quantify cognitive deficits and enhancements separately within the same model.
This is because the task-specific and task-general estimates are statistically independent.
Thus, for instance, we may find that adversity lowers general drift rate, as well as some task-specific drift rate (e.g., capturing inhibition), but increases other task-specific drift rates (e.g., capturing attention shifting).

If the drift rates we observe align with previous interpretations of performance differences as outlined above, our findings support existing theories about deficits and enhancements. 
However, if not drift rates, but non-decision time or boundary separation account for the existing findings, and drift rates do not, neither deficit- or adaptation-based frameworks are supported. 
This would at a minimum invite reflection---perhaps revision---of the evidence base for (parts of) these frameworks.
At the same time, such findings would offer clear directions for future research in this field (e.g., which factors explain variation in non-decision times and/or boundary separation across levels of adversity).
Thus, regardless of the specific pattern of outcomes, our analyses will contribute to an accurate and refined understanding of how early-life adversity shapes cognitive abilities.

# Methods {#methods}

## Sample {#meth_sample}

The ABCD study ([http://abcdstudy.org](http://abcdstudy.org)), is a prospective, longitudinal study of approximately 12,000 youth across the United States.
We focus on the baseline assessment, which has the largest collection of cognitive tasks suitable for DDM [@luciana_2018]. 
There are four tasks: (1) Processing Speed Task (Pattern Comparison Processing Speed Task), (2) Attention Shifting Task (Dimensional Change Card Sort Task), (3) Inhibition Task (Flanker Task), and (4) Mental Rotation Task (Little Man Task).
At baseline, the study included 11,878 youths (aged between 9 and 10 years old, measured in months) recruited across 21 sites. 
The study used multi-stage probability sampling to obtain a nationally representative sample [@heeringa_2010]. 
Baseline assessments were completed between September 1^st^ 2016 and August 31^st^ 2018 [see @garavan_2018].
Our analysis sample includes `r descriptives$precleaning_n$full_n_trial` participants who had trial-level data available on all four^2^ cognitive tasks.
We will provide descriptive statistics of the youth included in the final sample (i.e., age, income-to-needs ratio, parent education level, ethnicity) in Stage 2 of this Registered Reports submission.

## Open Science Statement {#meth_os}

All analysis scripts, materials, and instructions needed to reproduce the findings are available on the article's Github repository [(https://anonymous.4open.science/r/anon-255D/README.md)](https://anonymous.4open.science/r/anon-255D/README.md). 
The raw study data cannot be shared on public repositories.
Personal access to the ABCD dataset is required to fully reproduce our analyses and can be requested at [https://nda.nih.gov](https://nda.nih.gov).

We obtained access to the full ABCD data repository and performed initial data cleaning and analyses *prior* to Stage 1 submission.
However, we preprocessed cognitive task data in isolation to prevent biasing the analyses involving independent variables.
The goal of these analyses was to assure that the pre-selected cognitive tasks adhered to basic DDM assumptions and had the required trial-level data available in the right format.
These initial analyses were preregistered ([https://anonymous.4open.science/r/anon-255D/preregistrations/2022-09-20_preregistration_DDM.md](https://anonymous.4open.science/r/anon-255D/preregistrations/2022-09-20_preregistration_DDM.md)).

To increase transparency, we developed an automated workflow (using R and Git) to track the data files read into the analysis environment.
First-time access to any data file was automatically tracked via Git, providing an overview including the timestamp, a description of the data, and the R code that was used to read in the data.
The supplemental materials provide a detailed description and visual overview of this workflow.
An overview of the data access history is provided in the repository's README file ([https://anonymous.4open.science/r/anon-255D/README.md](https://anonymous.4open.science/r/anon-255D/README.md)).

## Exclusion Criteria {#meth_exclusions}

For the cognitive task data, we applied exclusion criteria in two steps: first, cleaning trial-level data, and second, removing participants with problematic trial-level data (discussed below).
For both, most criteria were as preregistered, but a few deviated from or were additional to the preregistration.
The data processing steps described below were preregistered unless noted otherwise.

First, we removed RTs of the Attention Shifting, Flanker, and Mental Rotation Tasks that exceeded maximum task-specific RT thresholds (\> 10 seconds (`r exclusions$dccs_trial$ex_RT_above_10`%), \> 10 seconds (`r exclusions$flanker_trial$ex_RT_above_10`%), and  \> 5 seconds (\< 0.01% of trials), respectively).
The Processing Speed Task did not have a programmed time-out.
Instead, we cut-off responses \> 10 seconds (`r exclusions$pcps_trial$ex_RT_above_10`% of trials) to remove extreme outliers.
This step was not preregistered as we did not anticipate these extreme outliers.

Next, we removed trials with: (1) RTs \< 300 ms (ranging from `r min(as.numeric(c(exclusions$lmt_trial$ex_fast_RT,exclusions$flanker_trial$ex_fast_RT,exclusions$dccs_trial$ex_fast_RT,exclusions$pcps_trial$ex_fast_RT,exclusions$picvoc_trial$ex_fast_RT)))`% to `r max(as.numeric(c(exclusions$lmt_trial$ex_fast_RT,exclusions$flanker_trial$ex_fast_RT,exclusions$dccs_trial$ex_fast_RT,exclusions$pcps_trial$ex_fast_RT,exclusions$picvoc_trial$ex_fast_RT)))`% of trials across tasks); (2) RTs \> 3 *SD* above the participant-level average log-transformed mean RT (ranging from `r min(as.numeric(c(exclusions$lmt_trial$ex_slow_RT,exclusions$flanker_trial$ex_slow_RT,exclusions$dccs_trial$ex_slow_RT,exclusions$pcps_trial$ex_slow_RT,exclusions$picvoc_trial$ex_slow_RT)))`% to `r max(as.numeric(c(exclusions$lmt_trial$ex_slow_RT,exclusions$flanker_trial$ex_slow_RT,exclusions$dccs_trial$ex_slow_RT,exclusions$pcps_trial$ex_slow_RT,exclusions$picvoc_trial$ex_slow_RT)))`% of trials across tasks; the same thing was done for RTs < 3 SD on the Processing Speed Task (not preregistered) to remove several fast outliers); (3) trials with missing response times and/or accuracy data (\< 0.01% for all tasks except Mental Rotation). We found that the response time-out of 5 seconds on the Mental Rotation Task led to missing responses on `r exclusions$lmt_trial$ex_missing_response`% of trials. 
This truncated the right-hand tail of the RT distribution, which can bias DDM estimation. 
Therefore, we decided to impute these values during DDM estimation instead of removing them (see the Supplemental materials for more information).

Next, we excluded participants who (1) had suffered possible mild traumatic brain injury or worse (*n* = `r smallNum(exclusions$lmt_participant$ex_tbi)`); (2) showed a response bias of \> 80% on a task (ranging between `r smallNum(min(as.numeric(c(exclusions$lmt_participant$ex_response_biases,exclusions$flanker_participant$ex_response_biases,exclusions$pcps_participant$ex_response_biases,exclusions$dccs_participant$ex_response_biases))))` and `r smallNum(max(as.numeric(c(exclusions$lmt_participant$ex_response_biases,exclusions$flanker_participant$ex_response_biases,exclusions$pcps_participant$ex_response_biases,exclusions$dccs_participant$ex_response_biases))))`; deviating from the preregistration); (3) had a low number of trials left after trial-level exclusions, defined as \< 20 trials for Mental Rotation and Attention Shifting (*n* = `r smallNum(exclusions$lmt_participant$ex_below_20_trials)` and `r smallNum(exclusions$dccs_participant$ex_below_20_trials)`, respectively) and \< 15 trials for Flanker and Processing Speed (*n* = `r exclusions$flanker_participant$ex_below_15_trials` and `r smallNum(exclusions$pcps_participant$ex_below_15_trials)`, respectively, deviating from the preregistration).
Finally, we excluded task data of several participants based on data inspection (not preregistered): `r smallNum(exclusions$lmt_participant$ex_0_accuracy)` participant with 0% accuracy on the Mental Rotation Task; `r smallNum(exclusions$pcps_participant$ex_decreasing_effort)` participants who showed a sharp decline in accuracy over time on the Processing Speed Task; `r smallNum(exclusions$dccs_participant$ex_switching_bias)` participants on the Attention Shifting Task who (almost) only made switches across all trials, even on repeat trials.
We also decided to include participants with missing data on one or more tasks because our main analyses will use FIML for missing data.

The final sample consisted of `r descriptives$postcleaning_n$full_n_trial` participants.

## Measures {#meth_measures}

### Cognitive Tasks {#meth_cogtasks}

**Flanker Task.** The NIH Toolbox Flanker task is a measure of cognitive control and attention [@zelazo_2014].
On each trial, participants saw five arrows that were positioned side-by-side.
The four flanking arrows always pointed in the same direction, either left or right.
The central arrow either pointed in the same direction (congruent trials) or in the opposite direction (incongruent trials).
Participants were instructed to always ignore the flanking arrows and to indicate whether the central arrow is pointing left or right.
After four practice trials, participants completed 20 test trials, of which 12 were congruent (*Mean~RT~* = `r descriptives$flanker$mean_rt_congruent` seconds, *SD* = `r descriptives$flanker$sd_rt_congruent`) and eight were incongruent (*Mean~RT~* = `r descriptives$flanker$mean_rt_incongruent` seconds, *SD* = `r descriptives$flanker$sd_rt_incongruent`).
The standard outcome measure is a normative composite of accuracy and RT. 
For more information on the exact calculation, see @slotkin_2012. 

**Processing Speed Task.** The NIH Toolbox Pattern Comparison Processing Speed task [@carlozzi_2015] is a measure of visual processing.
On each trial, participants saw two images and judged whether the images were the same or different.
When images were different, they varied on one of three dimensions: color, adding or taking something away, or containing more or less of a particular item.
The standard outcome measure is the number of items answered correctly in 90 seconds (normalized).
On average, participants completed `r pcps_clean |> count(subj_idx) |> summarise(n = mean(n)) |> summarise(mean = mean(n)) |> pull(mean) |> round(2)` trials (*Mean~RT~* = `r descriptives$pcps$mean_rt` seconds, *SD* = `r descriptives$pcps$sd_rt`).

**Attention Shifting Task.** The NIH Toolbox Dimensional Change Card Sort Task is a measure of attention shifting or cognitive flexibility [@zelazo_2006; @zelazo_2014].
A white rabbit and green boat were presented at the bottom of the screen.
Participants matched a third object to the rabbit or boat based on either color or shape.
After eight practice trials, participants completed 30 test trials alternating between shape and color in pseudo-random order.
Of these, 23 were *repeat* trials (i.e., the sorting rule was the same as on the previous trial; *Mean~RT~* = `r descriptives$dccs$mean_rt_repeat` seconds, *SD* = `r descriptives$dccs$sd_rt_repeat`) and 7 were *switch* trials (i.e., the sorting rule was different than on the previous trial; *Mean~RT~* = `r descriptives$dccs$mean_rt_switch` seconds, *SD* = `r descriptives$dccs$sd_rt_switch`).
The standard outcome measure is a normative composite of accuracy and RT.
For more information on the exact calculation, see @slotkin_2012. 

**Mental Rotation Task.** The Little Man task (referred to in this article as the Mental Rotation task) is a measure of visual-spatial processing [@luciana_2018].
Participants saw a simple picture of a male figure holding a briefcase in his left or right hand.
They had to indicate whether the briefcase was in the left or right hand.
The image could have one of four orientations: right side up or upside down, and facing towards or away from the participant.
Thus, on half of the trials, participants had to mentally rotate the image in order to make the decision.
Participants first completed three practice trials and then completed 32 test trials (*Mean~RT~* = `r descriptives$lmt$mean_rt`, *SD* = `r descriptives$lmt$sd_rt`).
The standard outcome measure is an efficiency measure, calculated as the percentage correct divided by the average RT.

### Adversity measures {#meth_adversity}

**Material deprivation.** We will assess material deprivation with seven items from the parent-reported ABCD Demographics Questionnaire.
These items originate from the Parent-Reported Financial Adversity Questionnaire [@diemer_2012].
The items assess whether or not (1 = Yes, 0 = No) the youth's family experienced several economic hardships over the 12 months prior to the assessment (e.g., 'Needed food but couldn't afford to buy it or couldn't afford to go out to get it').

We will use a previously created factor score of this measure derived from MNLFA [@bauer_2017].
This score empirically adjusts for measurement non-invariance across sociodemographic characteristics and creates person-specific factor scores that enhance measurement precision and individual variation [@curran_2014].
In short, MNLFA scores assume a common scale of measurement across groups and age, as well as adjust for measurement biases that would have otherwise biased our substantive analyses.
@dejoseph_2022 describe how this score was computed.
Higher scores indicate more material deprivation.

**Household threat.** We will assess threat experienced in the youth's home using the Family Conflict subscale of the ABCD Family Environment Scale [@moos_1994; @zucker_2018].
The subscale consisted of nine items assessing conflict with family members (e.g., 'We fight a lot in our family').
Items were endorsed with either 1 (True) or 0 (False).
Two items are positively valenced and will therefore be reverse-scored.
Similar to material deprivation, we will use a previously-created factor score of this measure derived from MNLFA [@dejoseph_2022].
Higher scores indicate more threat exposure.

**Sociodemographic covariates.** Several sociodemographic covariates will be included in the SEM models (see [Planned Main Analyses](#meth_proposed)) that use the MNLFA scores representing material deprivation and household threat exposure.
This is because MNLFA scores are adjusted for these covariates.
Thus, it is recommended that variation in these covariates is also adjusted for in dependent variables [@bauer_2017].

We will calculate income-to-needs ratios by first taking the average of each binned income (\< \$5000, \$5,000--\$11,999, \$12,000--\$15,999, \$16,000--\$24,999, \$25,000--\$34,999, \$35,000--\$49,999, \$50,000--\$74,999, \$75,000--\$99,999, \$100,000--\$199,999, \≥ \$200,000) as a rough approximation of the family's total reported income.
Then we will divide income by the federal poverty threshold for the year at which a family was interviewed (range = \$12,486--\$50,681), adjusted for the number of persons in the home.
We will use highest education (in years) out of the two caregivers (or one if a second caregiver was not provided) as a continuous variable.
We will collapse youth race into 4 levels (White, Black, Hispanic, Other) and subsequently dummy-code with White (the most numerous racial group) serving as the reference category in all models.
We will dichotomize youth sex such that 1 = Female and 0 = Male.
We will use youth age (in months) as a continuous variable and centered on the mean.

## Proposed Analysis Pipeline {#meth_analyses}

### Planned main analyses {#meth_proposed}

Before conducting analyses, we will split the full sample up in a training set (*n* = 1,500) and a test set (*n* \≈ 8,500).
We will conduct our main analyses in three steps (each discussed in more detail below): (1) fit the DDM to the cognitive task data; (2) fit the SEM model to the adversity and DDM data and optimize it where necessary based on the training set; (3) Refit the model to the test data and interpret the regression coefficients.
We conducted a simulation-based power analysis based on the main SEM model (see Figure 3), with standardized regression coefficients of 0.06, 0.08 and 0.1 and the alpha level set to .05.
The analysis indicated that we will have more than 90% power for all regression paths with *N* between 2,500 ($\beta$ = 0.1) and 6,500 ($\beta$ = 0.06).

All analyses will be conducted in R 4.2.1 [@Rcoreteam_2022].
The source code can be found on the Github repository ([https://anonymous.4open.science/r/anon-255D/scripts](https://anonymous.4open.science/r/anon-255D/scripts)).

<br>
<br>
[Figure 2 about here]
<br>
<br>

**Step 1: DDM estimation.** The DDM will be fit to each cognitive task in a hierarchical Bayesian framework which estimates DDM parameters both on the individual and group level [@vandekerckhove_2011; @wiecki_2013].
We use code provided by @johnson_2017.
The benefit of this approach is that group-level information is leveraged to estimate individual-level estimates.
This differs from classic DDM estimation approaches where the model is fitted to the data of each participant separately [@voss_2013].
This is particularly useful in developmental samples like the ABCD dataset which have a limited number of trials per participant but substantially larger sample sizes than is typical in the DDM literature^3^.

All models will freely estimate the drift rate, non-decision time, and boundary separation while constraining response bias to 0.5 (i.e., assuming no bias towards a particular response option).
For the Flanker and Attention Shifting Task, we will compare model versions that separately estimate drift rate and non-decision-time per task condition or collapsed across conditions.
Boundary separation will be constrained to be the same across conditions.
For the Processing Speed Task and the Mental Rotation Task, we estimate DDM parameters across all trials.
The best-fitting model of each task will be used to estimate participant-level DDM parameters.
See the supplement for more information about model fitting procedures.

**Step 2: Model optimization in training set.** We will first estimate and (where necessary) optimize the SEM in the training set using the *lavaan* package [@rosseel_2012].
This goal of this step is to investigate whether we need to adjust the model specification in any way (e.g., add residual correlations, introduce or reduce constraints of factor loadings, etc.) to achieve good model fit.
For this reason, the model fitted in this step will not be interpreted to address our research aims.

See Figure 3 for the *a-priori* specification of the model.
In the measurement model, all three DDM parameters across all tasks (i.e., drift rates, non-decision times, and boundary separations) will load on separate latent factors for each parameter type.
Unique (residual) variances of the manifest (i.e., measured) DDM parameters will be captured in additional latent factors (one per parameter).
Age will be added as a covariate to each manifest DDM parameter.
The structural model will estimate regression paths going from each adversity measure (see [Adversity measures](#meth_adversity)) to the general latent factors and to the unique variances of the DDM parameters of each task.
For model identification reasons, we will not estimate regression paths to the unique variances of the Processing Speed Task.
We will first estimate and optimize the measurement models separately for each diffusion model parameter, which will allow us to efficiently detect sources of potential badness of fit.
Once measurement models provide an adequate account of the data, we will integrate them into the structural model shown in Figure 3.
In addition, clustering of siblings and twins within families will be accounted for using the *lavaan.survey* package [@oberski_2014].
Finally, the sociodemographic covariates that are included in the MNLFA scores (see Measures section above) will be controlled for in the SEM. 
Goodness-of-fit will be assessed using the root mean square error of approximation (RMSEA) and the comparative fit index (CFI).
Following @hu_1999, CFI values \> .90 and RMSEA values \< .08 will be interpreted as acceptable model fit and CFI values \> .95 and RMSEA values \≤ .06 as good model fit.

<br>
<br>
[Figure 3 about here]
<br>
<br>

**Step 3: Model validation in test set.** After optimizing the model based on the training set, we will refit it to the test data.
Model fit will be assessed the same way as at Step 2.
The regression coefficients of these models will be interpreted to address our research questions.
We will control for multiple testing in the regression paths based on the false discovery rate [@benjamini_1995; @cribbie_2007].
We will do so separately for tests involving drift rates, non-decision times, and boundary separations, as we have different hypotheses for each of these parameters.
In addition, we are interested in determining if effects that fall between -.10 and .10 are consistent with an actual null effect.
For regression coefficients falling within these bounds, we therefore use two one-sided tests (TOST) equivalence testing using -.10 and .10 as bounds.

## Timeline

All data required for the study have already been collected.
We estimate we will need four months after Stage 1 in-principle acceptance to fit all the DDM models, run the analyses, and finish the Stage 2 report.

\pagebreak

# Footnotes

^1^ A fourth DDM parameter,, the *starting point* (*z*), represents an initial bias towards one of the two decision options (e.g., a tendency to classify facial expressions as angry that extends to neutral faces).
Note that allowing the starting point to vary only makes sense if response options differ in valence (e.g., happy and angry faces, which the current study does not include and thus is unable to examine).

^2^ The preregistration also included the Picture Vocabulary Task.
However, after accessing the data we realized that this task was implemented using computerized adaptive testing [@luciana_2018].
This makes it unsuitable for DDM, as the model assumes the level of difficulty is the same across trials.

^3^ We ran parameter recovery studies simulating the data for the Flanker Task, which has the lowest overall number of trials. Parameter recovery was excellent for the scenario that we plan in our main analyses (all *r*s \≥ .84). See the supplemental materials for more details.

\pagebreak

# References

::: {#refs}
:::

\pagebreak

```{r figure1, fig.width=5.5, dpi=600, fig.id = "figure1", fig.cap.style = "Image Caption", fig.cap='**Figure 1.** A visual overview of the Drift Diffusion Model (DDM). The DDM assumes that decision making on cognitive tasks with two forced response options advances through three stages. First, people go through a preparation phase in which they engage in initial stimulus encoding. Second, people gather information for one of two response options until the accumulation process terminates at one of the decision boundaries. Each squiggly line  represents the evidence accumulation process on a single trial. Third, a motor response is triggered in the execution phase. The model estimates four parameters that reflect distinct cognitive processes (printed in italic): (1) The *drift rate* represents the rate at which evidence accumulation drifts towards the decision boundary and is a measure of processing speed; (2) The *non-decision time* represents the combined time spent on task preparation and response execution; (3) The *boundary separation* represents the width of the decision boundaries and is a measure of response caution; (4) The *starting point* represents the starting point of the decision process and can be used to model response biases (not considered in this study).'}
knitr::include_graphics("images/fig1.png")
```


\pagebreak


```{r figure2, fig.width=5, dpi=600, fig.id = "figure2", fig.cap.style = "Image Caption", fig.cap='**Figure 2.** Visual overview of the full analysis workflow. Analyses are done in two stages: (1) prior to Stage 1 submission of the manuscript, and (2) after Stage 1 in-principle acceptance. Analyses at Stage 1 only focus on the cognitive task data. Independent variables (i.e., threat and deprivation measures) will only be accessed during Stage 2 after all DDM models have been fit, and only for the test set after the model has been optimized based on the training set. Data access will be tracked via the GitHub repository. IV = independent variable; SEM = structural equation modeling; DDM = Drift Diffusion Model.'}
knitr::include_graphics("images/fig2.png")
```

\pagebreak

```{r figure3, fig.width=6, dpi=600, fig.id = "figure3", fig.cap.style = "Image Caption", fig.cap='**Figure 3.** Visualization of the full structural equation model (SEM). Dotted black lines represent covariances. Dashed black lines represent factor loadings. Solid grey lines represent regression paths. The factor loadings to each of the Processing Speed Task indicators are fixed to 1. The unique variances of each manifest indicator are captured in additional latent factors (U), one per indicator. To this end, the factor loadings are fixed to 0 and the residual variances of the manifest indicators are fixed to 0. For model identification reasons, we do not estimate regression paths to the unique variances of the Processing Speed Task. Not shown in this Figure to improve readability: (1) the sociodemographic covariates that are included in the MNLFA scores (see Measures section), and (2) age as a covariate for the manifest DDM parameters. PS = Processing Speed Task; AS Rep = Attention Shifting Task repeat trials; AS Sw = Attention Shifting Task switch trials; MR = Mental Rotation Task; FL Con = Flanker Task congruent trials; FL Inc = Flanker Task incongruent trials; *v* = Drift rate; *a* = Boundary separation; *t0* = Non-decision time; U = unique variance.'}
knitr::include_graphics("images/fig3.png")
```

---
bibliography: references.bib
csl: apa.csl
format: 
  docx:
    reference-doc: reference-doc.docx
output:
  officedown::rdocx_document:
    page_margins:
      bottom: 1
      footer: 0
      gutter: 0
      header: 0.5
      left: 1
      right: 1
      top: 1
    plots:
      align: center
      caption:
        pre: 'Figure '
        sep: '. '
        style: Image Caption
    tables:
      caption:
        pre: 'Table '
        sep: '. '
        style: Table Caption
  pdf_document: default
  word_document: default
editor: 
  markdown: 
    wrap: sentence
---

```{r include = FALSE}
library(flextable)
library(stringr)
library(dplyr)
library(english)

source("../scripts/custom_functions/general-functions.R")
load("staged_results.RData")


knitr::opts_chunk$set(
  echo = F,
  fig.align = "center",
  fig.pos = "!t", 
  out.extra = "",
  fig.show = "asis",
  message = FALSE,
  tab.topcaption = T,
  warning = FALSE
)

# set up flextable for tables
set_flextable_defaults(
  font.family = "Times", 
  font.size = 10,
  font.color = "black",
  line_spacing = 1,
  padding.bottom = 1, 
  padding.top = 1,
  padding.left = 1,
  padding.right = 1
)

```

#### **Cognitive deficits and enhancements in youth from adverse conditions: An integrative assessment using Drift Diffusion Modeling in the ABCD study**

<br>

#### Stefan Vermeent^1,2^, Ethan S. Young^1^, Meriah L. DeJoseph^3^, Anna-Lena Schubert^4^, & Willem E. Frankenhuis^1,2^

#### ^1^ Department of Psychology, Utrecht University, Utrecht, The Netherlands

#### ^2^ Max Planck Institute for the Study of Crime, Security, and Law, Freiburg, Germany

#### ^3^ Institute of Child Development, University of Minnesota, USA

#### ^4^ Department of Psychology, University of Mainz, Mainz, Germany

<br>

# Data Availability

All scripts and materials needed to reproduce the findings are available on the article's Github repository [(https://anonymous.4open.science/r/anon-255D/README.md)](https://anonymous.4open.science/r/anon-255D/README.md).
We also include instructions on how to reproduce each step of our analyses.
To ensure computational reproducibility, we provide synthetic (i.e., simulated) data files with the same characteristics as the raw data.
The raw data supporting the findings of this study will be made available upon after Stage 2 via [https://doi.org/10.15154/1528297](https://doi.org/10.15154/1528297).

Data used in the preparation of this article were obtained from the Adolescent Brain Cognitive Development^SM^ (ABCD) Study (https://abcdstudy.org), held in the NIMH Data Archive (NDA). 
This is a multisite, longitudinal study designed to recruit more than 10,000 children age 9-10 and follow them over 10 years into early adulthood. 
The ABCD Study® is supported by the National Institutes of Health and additional federal partners under award numbers U01DA041048, U01DA050989, U01DA051016, U01DA041022, U01DA051018, U01DA051037, U01DA050987, U01DA041174, U01DA041106, U01DA041117, U01DA041028, U01DA041134, U01DA050988, U01DA051039, U01DA041156, U01DA041025, U01DA041120, U01DA051038, U01DA041148, U01DA041093, U01DA041089, U24DA041123, U24DA041147. 
A full list of supporters is available at [https://abcdstudy.org/federal-partners.html](https://abcdstudy.org/federal-partners.html). 
A listing of participating sites and a complete listing of the study investigators can be found at [https://abcdstudy.org/consortium_members/](https://abcdstudy.org/consortium_members/). 
ABCD consortium investigators designed and implemented the study and/or provided data but did not necessarily participate in the analysis or writing of this report. 
This manuscript reflects the views of the authors and may not reflect the opinions or views of the NIH or ABCD consortium investigators.
The ABCD data repository grows and changes over time. 
The ABCD data used in this report came from Data Release 4.0 (DOI: [http://dx.doi.org/10.15154/1523041](http://dx.doi.org/10.15154/1523041)).

# Funding Statement
WEF’s contributions have been supported by the Dutch Research Council (V1.Vidi.195.130) and the James S. McDonnell Foundation (https://doi.org/10.37717/220020502). 


# Disclosures
We declare no conflicts of interest.

# Ethics Approval Statement

This study was approved by the Ethics Review Board of the Faculty of Social & Behavioural Sciences of Utrecht University (FETC20-490).

\pagebreak

# Proposal Research Highlights

1. We use Drift Diffusion Modeling (DDM) to investigate how two forms of early-life adversity---material deprivation and household threat---are associated with lowered or improved cognitive performance.
2. The DDM can inform us if and where cognitive differences occur along distinct stages of cognitive processing.
3. We will also use structural equation modeling and out-of-sample validation to tease apart effects of adversity that are task-specific versus task-general.
4. We will apply our approach to a large, representative sample of around 10,000 9- to 10 year-olds from the ABCD study.

\pagebreak

# Proposal Abstract

Childhood adversity can lead to either cognitive deficits or enhancements, depending on many factors. 
Though progress has been made, two challenges prevent us from integrating and better understanding these findings. 
First, studies commonly use and interpret raw performance differences, such as mean response times or overall accuracy. 
However, raw scores conflate different stages of cognitive processing. 
Second, studies tend to either isolate or aggregate abilities, obscuring the degree to which individual differences reflect task-specific or task-general processes. 
We address these challenges using Drift Diffusion Modeling (DDM) and structural equation modeling.
Combining these techniques, we can (1) relate early-life adverse experiences to individual differences in different stages of processing, and (2) investigate whether these reflect differences in specific or task-general performance. 
We examine how two forms of adversity---material deprivation and household threat---affect performance on four cognitive tasks in a large, representative sample of 9-10 year-olds from the Adolescent Brain Cognitive Development (ABCD) study. 
This approach holds promise for both deficit- and strength-based research questions. 
It will add much-needed nuance to adversity-related performance differences, which can inform theory and intervention.

*Keywords:* adversity, cognitive deficits, cognitive enhancements, drift diffusion modeling, Adolescent Brain Cognitive Development (ABCD) Study 

\pagebreak

#### Cognitive deficits and enhancements in youth from adverse conditions: An integrative assessment using Drift Diffusion Modeling in the ABCD study

<br>

The effects of early-life adversity---such as growing up in poverty or experiencing high levels of violence---on cognition are complex.
There is a growing consensus that adversity-exposed youth may develop not only deficits, but also strengths.
For example, studies find deficits and strengths across different cognitive domains including (but not limited to) executive functioning, social cognition, language, and emotion [@ellis_2022; @frankenhuis_2013; @frankenhuis_2016;  @sheridan_2022; @sheridan_2014].
Researchers focused on one type of effect or another acknowledge the importance of identifying both deficits and strengths.
Yet, in practice, they often focus on one at the expense of the other.
To develop an integrated, well-rounded, and nuanced understanding of how adversity shapes cognitive abilities, research must integrate both types of effects.

Such an integration of deficit- and strength-based approaches is hampered by two methodological challenges.
First, most cognitive tasks involve different stages of processing which are obscured when analyzing raw performance differences.
This makes it difficult to understand why cognitive performance may be lowered or improved. 
Second, adversity may lower or improve performance because it affects general processes (i.e., processes shared across many tasks) or abilities that are task-specific.
In this Registered Report, we use a framework that tackles both challenges.
First, we decompose raw performance into measures of different stages of cognitive processes through cognitive modeling.
Second, we analyze four different tasks---tapping processing speed, attention shifting, inhibition, and mental rotation---most of which have shown associations with adversity.
Finally, we model shared (i.e., task-general) and unique (i.e., task-specific) factors that drive performance and investigate how they are associated with adversity.

## What do deficit and enhancement patters mean? {#intro_sub1}

Both the deficit and strength-based literature routinely use speeded tasks, in which participants are usually instructed to respond as fast and accurate as possible.
For example, performing well on inhibition tasks [e.g., Flanker task, Go/No-Go Task\; @farah_2006; @fields_2021; @mezzacappa_2004; @noble_2005], attention shifting tasks [e.g., Dimensional Change Card Sort\; @farah_2006; @fields_2021; @nweze_2021; @young_2022; @mittal_2015; @noble_2005], and stimulus detection tasks [@farah_2006; @noble_2005; @pollak_2008] requires fast and accurate responses.
In practice, performance is often quantified using aggregated indices of speed alone (e.g., RT), accuracy alone (e.g., proportion correct), or both independently (rather than in an integrated manner).

In both the deficit and strength-based literature, *task performance* (indexed by mean RTs or accuracy) is routinely equated with *cognitive ability*.
For example, deficit-focused studies relate slower RTs on inhibition tasks to *worse inhibition ability* [@farah_2006; @fields_2021; @mezzacappa_2004; @noble_2005].
Strength-based studies relate faster RTs on standard attention shifting tasks to *better shifting ability* [@fields_2021; @young_2022; @mittal_2015].
However, speed and accuracy comprise more than pure ability (e.g., inhibition, attention shifting).
They also measure other constructs such as response caution (e.g., more or less cautious responding), speed of task preparation (e.g., orienting attention, encoding the stimuli), and speed of response execution. 
This heterogeneity creates an inferential risk, namely, if performance differences are interpreted as differences in abilities without sufficiently considering alternative explanations.
In addition, the effect of adversity exposure may not be limited to a single process.
For example, a specific type of adversity could affect both the speed of information processing and also shape the strategy that a person uses.
These inferential challenges have real-world implications, especially when raw performance is used as an early screening tool to assess cognitive abilities [@distefano_2021].

One promising solution to these issues is leveraging cognitive measurement models developed by mathematical psychologists.
For speeded binary decision tasks, a well-established measurement model is the Drift Diffusion Model [DDM\; @forstmann_2016; @ratcliff_1998; @ratcliff_2008; @wagenmakers_2009].
The DDM integrates speed and accuracy on a trial-by-trial level to estimate cognitive processes at different stages of the decision-making process.
The DDM assumes that people go through three distinct phases of cognitive processing on each trial (see Figure S1 for a visualization).
The first phase, *preparation*, includes processes such as focusing attention and visually encoding the stimulus.
In the second phase, *decision-making*, people gather evidence for both response options until the evidence sufficiently favors one option over the other (explained below) and the decision process terminates. 
The third phase, *execution*, involves preparing and executing the motor response corresponding to the choice.

```{r figure1, fig.width=5.5, dpi=600, fig.id = "figure1", fig.cap.style = "Image Caption", fig.cap='**Figure 1.** A visual overview of the Drift Diffusion Model (DDM). The DDM assumes that decision making on cognitive tasks with two forced response options advances through three stages. First, people go through a preparation phase in which they engage in initial stimulus encoding. Second, people gather information for one of two response options until the accumulation process terminates at one of the decision boundaries. Each squiggly line  represents the evidence accumulation process on a single trial. Third, a motor response is triggered in the execution phase. The model estimates four parameters that reflect distinct cognitive processes (printed in italic): (1) The *drift rate* represents the rate at which evidence accumulation drifts towards the decision boundary and is a measure of processing speed; (2) The *non-decision time* represents the combined time spent on task preparation and response execution; (3) The *boundary separation* represents the width of the decision boundaries and is a measure of response caution; (4) The *starting point* represents the starting point of the decision process and can be used to model response biases (not considered in this study).'}
knitr::include_graphics("images/fig1.png")
```

<br>

DDM estimates a set of parameters for each participant that represent each phase of the decision process [@voss_2004]. 
The *drift rate* (*v*) represents the speed of information uptake [@schmiedek_2007; @voss_2013]. 
People with a higher drift rate are faster and make fewer errors. 
The *non-decision time* (*t0*) includes initial preparatory processes (e.g., visually encoding the stimulus) and processes after the decision is made (e.g., pressing a button). 
All else being equal, longer non-decision times reflect slower information processing but without a cost nor benefit in accuracy.
The *boundary separation* (*a*) represents the distance between the two decision boundaries.
A larger boundary separation means more information is collected before making a decision.
Thus, boundary separation measures response caution.
In contrast to non-decision time, larger boundary separation leads to slower but more accurate responses, reflecting a speed-accuracy tradeoff. 
Finally, the *starting point* (*z*) represents an initial bias towards one of the two decision options (e.g., a tendency to classify facial expressions as angry that extends to neutral faces).
Note that allowing the starting point to vary only makes sense if response options differ in valence (e.g., happy and angry faces, which the current study does not include and thus is unable to examine).
Out of the four DDM parameters, the drift rate in most cases captures the variation in the cognitive ability of interest (e.g., inhibition, attention shifting).
That is because a better functioning of the ability of interest should both increase speed of processing as well as reduce errors.
Thus, disentangling the drift rate, non-decision time, and boundary separation enhances our understanding of how adversity-exposure is associated with performance.

## Are deficit and enhancement patterns task-specific or task-general? {#intro_sub2}

Performance on cognitive tasks often relies both on shared cognitive processes and unique abilities.
For example, RTs on executive functioning tasks are substantially confounded with general processing efficiency [@frischkorn_2019; @lerche_2020; @loffler_2022]. 
Both task-specific abilities and task-general processes affect RTs and accuracy in similar ways and are thus likely confounded in drift rates.
It is important to separate these effects, for both deficit- and strength-based approaches (e.g., does adversity impair broad domains such as executive functioning? Does it enhance specific abilities such as attention shifting?), as well as for real-world interventions grounded in both approaches (e.g., school-based interventions targeting broad  domains or specific abilities).

Structural equation modeling (SEM) can disentangle task-general and task-specific processes. 
For example, it can estimate shared task variance with latent task-general variables.
By estimating shared variance across different tasks, we can also obtain more precise estimates of task-specific abilities (i.e., variance unique to specific tasks).
@bignardi_2022 recently applied this approach to model how SES is related to standard performance measures in three large data sets.
They used SEM to model the effect of SES on a general factor and task-specific residual variances.
Lower SES was associated with a lower general ability, but *enhanced* task-specific processing speed, inhibition, and attention shifting.
However, their analysis looked at shared and unique variance using raw performance measures.
We extend this SEM analysis technique to DDM parameters.
Our approach affords insight into how task-general and task-specific drift rates, non-decision times, and boundary separations relate to adversity exposures.


## The current study {#intro_current}

In this study, we will use the Adolescent Brain Cognitive Development (ABCD) study data ([http://abcdstudy.org](http://abcdstudy.org)).
The ABCD study is ideal because it provides a large, representative, and socioeconomically and ethnically diverse sample of 9- to 10 year-olds---an age range characterized by rapid growth in cognitive abilities [@blakemore_2006].
We will fit one general SEM mapping two key dimensions of adversity---material deprivation and household threat---to task-general and task-specific DDM parameters representing processing speed, attention shifting, inhibition, and mental rotation.
The measures of adversity were previously adjusted for measurement non-invariance across sociodemographic characteristics in the ABCD sample using moderated nonlinear factor analysis [MNLFA\; @bauer_2017; @dejoseph_2022].

We include measures of threat and deprivation because these forms of adversity have been widely studied in their relation to cognitive outcomes from both deficit and strength-based perspectives [@fields_2021; @schafer_2022; @sheridan_2022; @young_2022] and are central to contemporary conceptualizations of adversity [e.g., @mclaughlin_2021; @sheridan_2014].
We include an Attention-Shifting task because previous work has found this ability to be enhanced in children and (young) adults with more environmental unpredictability [@fields_2021; @mittal_2015; @young_2022]. 
We include the Flanker Task because children with more adverse experiences typically show worse inhibition [e.g., @fields_2021; @tibu_2016]. 
We include the Mental Rotation Task because previous studies have found negative associations between SES and mental rotation ability [e.g., @assari_2020; @bignardi_2022].
Also, this task is widely used in developmental science and contributes unique variance that is not captured by any of the other cognitive tasks in our set.
Finally, we include the Processing Speed Task because it offers a purer measure of the type of basic processing speed that plays a role in the other tasks.
Generally, the decision to include these four tasks (and exclude others in the ABCD data) is guided by these tasks adhering to DDM assumptions.

Both the deficit- and strength-based literature provide general insights that guide our expectations about how different types of adversity may be associated with Drift Diffusion parameters.
First, a central assumption of strength-based frameworks is that specific types of adversity shape specific abilities that help youth solve real-world challenges [@ellis_2022; @frankenhuis_2013; @frankenhuis_2016; @frankenhuis_2020].
Following this assumption, we expect adversity-related enhancements to arise on the task-specific level, and not on the task-general level.
In addition, both household threat and material deprivation may lead to specific enhancements on different abilities, about which we do not have specific expectations.
Second, cognitive deficits have been demonstrated both for specific abilities [e.g., @fields_2021] and on a general level [e.g., @salhi_2021].
Moreover, previous studies demonstrate that cognitive deficits are more strongly associated with cognitive deprivation than with threat exposure [@salhi_2021; @sheridan_2020].
Although cognitive deprivation and material deprivation (as used here) are not the same, we assume that both are related to youth's access to resources to support cognitive development (e.g., books in the home and formal education).
This assumption is supported by work based on the ABCD sample showing that material deprivation is substantially correlated with income (-.81) and education (-.56), while correlations with household threat are low [-.25 and -.12, respectively\; @dejoseph_2022].
Therefore, we assume that the association between material deprivation and cognitive performance will be in the same (negative) direction as the association with cognitive deprivation.
These general insights lead us to the following expectations:

(1) Drift rate.
    (a) We expect any adversity-related improvements to manifest in task-specific drift rates and not in task-general drift rates. In particular, we expect adversity to be positively related to drift rates specific to attention shifting [@fields_2021; @mittal_2015; @young_2022; but see @nweze_2021].
    (b) We expect adversity to be associated with lower drift rates (i.e., deficit patterns) at both the task-general and task-specific level.
    (c) We expect drift rates at both of these levels to be more negatively associated with material deprivation than with household threat.

(2) Non-decision time. Existing theory offers little direction for predicting how adversity should relate to non-decision time, but we can conceive of two general patterns:
    (a) Both material deprivation and household threat may generally be associated with slower speed of attending to and encoding information or executing responses, leading to task-general deficits in non-decision times.
    (b) Youth with more exposure to household threat may develop a baseline hyper-vigilance, leading to faster task-specific non-decision times [@dejoseph_2022].

(3) Boundary separation. We do not have specific predictions for associations between adversity and boundary separation, as differences in boundary separations cannot be interpreted as either deficits or enhancements, but rather as strategies.

# Methods {#methods}

## Sample {#meth_sample}

The ABCD study ([http://abcdstudy.org](http://abcdstudy.org)), is a prospective, longitudinal study of approximately 12,000 youth across the United States.
We focus on the baseline assessment, which has the largest collection of cognitive tasks suitable for DDM [@luciana_2018]. 
There are four tasks: (1) Processing Speed Task (Pattern Comparison Processing Speed Task), (2) Attention Shifting Task (Dimensional Change Card Sort Task), (3) Inhibition Task (Flanker Task), and (4) Mental Rotation Task (Little Man Task).
At baseline, the study included 11,878 youths (aged between 9 and 10 years old, measured in months) recruited across 21 sites. 
The study used multi-stage probability sampling to obtain a nationally representative sample [@heeringa_2010]. 
Baseline assessments were completed between September 1^st^ 2016 and August 31^st^ 2018 [see @garavan_2018].
Our analysis sample includes `r descriptives$precleaning_n$full_n_trial` participants who had trial-level data available on all four^1^ cognitive tasks.
We will provide descriptive statistics of the youth included in the final sample (i.e., age, income-to-needs ratio, parent education level, ethnicity) in Stage 2 of this Registered Reports submission.

## Open Science Statement {#meth_os}

All analysis scripts, materials, and instructions needed to reproduce the findings are available on the article's Github repository [(https://anonymous.4open.science/r/anon-255D/README.md)](https://anonymous.4open.science/r/anon-255D/README.md). 
The raw study data cannot be shared on public repositories.
Personal access to the ABCD dataset is required to fully reproduce our analyses and can be requested at [https://nda.nih.gov](https://nda.nih.gov).

We obtained access to the full ABCD data repository and performed initial data cleaning and analyses *prior* to Stage 1 submission.
However, we preprocessed cognitive task data in isolation to prevent biasing the analyses involving independent variables.
The goal of these analyses was to assure that the pre-selected cognitive tasks adhered to basic DDM assumptions and had the required trial-level data available in the right format.
These initial analyses were preregistered ([https://anonymous.4open.science/r/anon-255D/preregistrations/2022-09-20_preregistration_DDM.md](https://anonymous.4open.science/r/anon-255D/preregistrations/2022-09-20_preregistration_DDM.md)).

To increase transparency, we developed an automated workflow (using R and Git) to track the data files read into the analysis environment.
First-time access to any data file was automatically tracked via Git, providing an overview including the timestamp, a description of the data, and the R code that was used to read in the data.
The supplemental materials provide a detailed description and visual overview of this workflow.
An overview of the data access history is provided in the repository's README file ([https://anonymous.4open.science/r/anon-255D/README.md](https://anonymous.4open.science/r/anon-255D/README.md)).

## Exclusion Criteria {#meth_exclusions}

For the cognitive task data, we applied exclusion criteria in two steps: first, cleaning trial-level data, and second, removing participants with problematic trial-level data (discussed below).
For both, most criteria were as preregistered, but a few deviated from or were additional to the preregistration.
The data processing steps described below were preregistered unless noted otherwise.

First, we removed RTs of the Attention Shifting, Flanker, and Mental Rotation Tasks that exceeded maximum task-specific RT thresholds (\> 10 seconds (`r exclusions$dccs_trial$ex_RT_above_10`%), \> 10 seconds (`r exclusions$flanker_trial$ex_RT_above_10`%), and  \> 5 seconds (\< 0.01% of trials), respectively).
The Processing Speed Task did not have a programmed time-out.
Instead, we cut-off responses \> 10 seconds (`r exclusions$pcps_trial$ex_RT_above_10`% of trials) to remove extreme outliers.
This step was not preregistered as we did not anticipate these extreme outliers.

Next, we removed trials with: (1) RTs \< 300 ms (ranging from `r min(as.numeric(c(exclusions$lmt_trial$ex_fast_RT,exclusions$flanker_trial$ex_fast_RT,exclusions$dccs_trial$ex_fast_RT,exclusions$pcps_trial$ex_fast_RT,exclusions$picvoc_trial$ex_fast_RT)))`% to `r max(as.numeric(c(exclusions$lmt_trial$ex_fast_RT,exclusions$flanker_trial$ex_fast_RT,exclusions$dccs_trial$ex_fast_RT,exclusions$pcps_trial$ex_fast_RT,exclusions$picvoc_trial$ex_fast_RT)))`% of trials across tasks); (2) RTs \> 3 *SD* above the participant-level average log-transformed mean RT (ranging from `r min(as.numeric(c(exclusions$lmt_trial$ex_slow_RT,exclusions$flanker_trial$ex_slow_RT,exclusions$dccs_trial$ex_slow_RT,exclusions$pcps_trial$ex_slow_RT,exclusions$picvoc_trial$ex_slow_RT)))`% to `r max(as.numeric(c(exclusions$lmt_trial$ex_slow_RT,exclusions$flanker_trial$ex_slow_RT,exclusions$dccs_trial$ex_slow_RT,exclusions$pcps_trial$ex_slow_RT,exclusions$picvoc_trial$ex_slow_RT)))`% of trials across tasks; the same thing was done for RTs < 3 SD on the Processing Speed Task (not preregistered) to remove several fast outliers); (3) trials with missing response times and/or accuracy data (\< 0.01% for all tasks except Mental Rotation). We found that the response time-out of 5 seconds on the Mental Rotation Task led to missing responses on `r exclusions$lmt_trial$ex_missing_response`% of trials. 
This truncated the right-hand tail of the RT distribution, which can bias DDM estimation. 
Therefore, we decided to impute these values during DDM estimation instead of removing them (see the Supplemental materials for more information).

Next, we excluded participants who (1) had suffered possible mild traumatic brain injury or worse (*n* = `r smallNum(exclusions$lmt_participant$ex_tbi)`); (2) showed a response bias of \> 80% on a task (ranging between `r smallNum(min(as.numeric(c(exclusions$lmt_participant$ex_response_biases,exclusions$flanker_participant$ex_response_biases,exclusions$pcps_participant$ex_response_biases,exclusions$dccs_participant$ex_response_biases))))` and `r smallNum(max(as.numeric(c(exclusions$lmt_participant$ex_response_biases,exclusions$flanker_participant$ex_response_biases,exclusions$pcps_participant$ex_response_biases,exclusions$dccs_participant$ex_response_biases))))`; deviating from the preregistration); (3) had a low number of trials left after trial-level exclusions, defined as \< 20 trials for Mental Rotation and Attention Shifting (*n* = `r smallNum(exclusions$lmt_participant$ex_below_20_trials)` and `r smallNum(exclusions$dccs_participant$ex_below_20_trials)`, respectively) and \< 15 trials for Flanker and Processing Speed (*n* = `r exclusions$flanker_participant$ex_below_15_trials` and `r smallNum(exclusions$pcps_participant$ex_below_15_trials)`, respectively, deviating from the preregistration).
Finally, we excluded task data of several participants based on data inspection (not preregistered): `r smallNum(exclusions$lmt_participant$ex_0_accuracy)` participant with 0% accuracy on the Mental Rotation Task; `r smallNum(exclusions$pcps_participant$ex_decreasing_effort)` participants who showed a sharp decline in accuracy over time on the Processing Speed Task; `r smallNum(exclusions$dccs_participant$ex_switching_bias)` participants on the Attention Shifting Task who (almost) only made switches across all trials, even on repeat trials.
We also decided to include participants with missing data on one or more tasks because our main analyses will use FIML for missing data.

The final sample consisted of `r descriptives$postcleaning_n$full_n_trial` participants.

## Measures {#meth_measures}

### Cognitive Tasks {#meth_cogtasks}

**Flanker Task.** The NIH Toolbox Flanker task is a measure of cognitive control and attention [@zelazo_2014].
On each trial, participants saw five arrows that were positioned side-by-side.
The four flanking arrows always pointed in the same direction, either left or right.
The central arrow either pointed in the same direction (congruent trials) or in the opposite direction (incongruent trials).
Participants were instructed to always ignore the flanking arrows and to indicate whether the central arrow is pointing left or right.
After four practice trials, participants completed 20 test trials, of which 12 were congruent (*Mean~RT~* = `r descriptives$flanker$mean_rt_congruent` seconds, *SD* = `r descriptives$flanker$sd_rt_congruent`) and eight were incongruent (*Mean~RT~* = `r descriptives$flanker$mean_rt_incongruent` seconds, *SD* = `r descriptives$flanker$sd_rt_incongruent`).
The standard outcome measure is a normative composite of accuracy and RT. 
For more information on the exact calculation, see @slotkin_2012. 

**Processing Speed Task.** The NIH Toolbox Pattern Comparison Processing Speed task [@carlozzi_2015] is a measure of visual processing.
On each trial, participants saw two images and judged whether the images were the same or different.
When images were different, they varied on one of three dimensions: color, adding or taking something away, or containing more or less of a particular item.
The standard outcome measure is the number of items answered correctly in 90 seconds (normalized).
On average, participants completed `r pcps_clean |> count(subj_idx) |> summarise(n = mean(n)) |> summarise(mean = mean(n)) |> pull(mean) |> round(2)` trials (*Mean~RT~* = `r descriptives$pcps$mean_rt` seconds, *SD* = `r descriptives$pcps$sd_rt`).

**Attention Shifting Task.** The NIH Toolbox Dimensional Change Card Sort Task is a measure of attention shifting or cognitive flexibility [@zelazo_2006; @zelazo_2014].
A white rabbit and green boat were presented at the bottom of the screen.
Participants matched a third object to the rabbit or boat based on either color or shape.
After eight practice trials, participants completed 30 test trials alternating between shape and color in pseudo-random order.
Of these, 23 were *repeat* trials (i.e., the sorting rule was the same as on the previous trial; *Mean~RT~* = `r descriptives$dccs$mean_rt_repeat` seconds, *SD* = `r descriptives$dccs$sd_rt_repeat`) and 7 were *switch* trials (i.e., the sorting rule was different than on the previous trial; *Mean~RT~* = `r descriptives$dccs$mean_rt_switch` seconds, *SD* = `r descriptives$dccs$sd_rt_switch`).
The standard outcome measure is a normative composite of accuracy and RT.
For more information on the exact calculation, see @slotkin_2012. 

**Mental Rotation Task.** The Little Man task (referred to in this article as the Mental Rotation task) is a measure of visual-spatial processing [@luciana_2018].
Participants saw a simple picture of a male figure holding a briefcase in his left or right hand.
They had to indicate whether the briefcase was in the left or right hand.
The image could have one of four orientations: right side up or upside down, and facing towards or away from the participant.
Thus, on half of the trials, participants had to mentally rotate the image in order to make the decision.
Participants first completed three practice trials and then completed 32 test trials (*Mean~RT~* = `r descriptives$lmt$mean_rt`, *SD* = `r descriptives$lmt$sd_rt`).
The standard outcome measure is an efficiency measure, calculated as the percentage correct divided by the average RT.

### Adversity measures {#meth_adversity}

**Material deprivation.** We will assess material deprivation with seven items from the parent-reported ABCD Demographics Questionnaire.
These items originate from the Parent-Reported Financial Adversity Questionnaire [@diemer_2012].
The items assess whether or not (1 = Yes, 0 = No) the youth's family experienced several economic hardships over the 12 months prior to the assessment (e.g., 'Needed food but couldn't afford to buy it or couldn't afford to go out to get it').

We will use a previously created factor score of this measure derived from MNLFA [@bauer_2017].
This score empirically adjusts for measurement non-invariance across sociodemographic characteristics and creates person-specific factor scores that enhance measurement precision and individual variation [@curran_2014].
In short, MNLFA scores assume a common scale of measurement across groups and age, as well as adjust for measurement biases that would have otherwise biased our substantive analyses.
@dejoseph_2022 describe how this score was computed.
Higher scores indicate more material deprivation.

**Household threat.** We will assess threat experienced in the youth's home using the Family Conflict subscale of the ABCD Family Environment Scale [@moos_1994; @zucker_2018].
The subscale consisted of nine items assessing conflict with family members (e.g., 'We fight a lot in our family').
Items were endorsed with either 1 (True) or 0 (False).
Two items are positively valenced and will therefore be reverse-scored.
Similar to material deprivation, we will use a previously-created factor score of this measure derived from MNLFA [@dejoseph_2022].
Higher scores indicate more threat exposure.

**Sociodemographic covariates.** Several sociodemographic covariates will be included in the SEM models (see [Planned Main Analyses](#meth_proposed)) that use the MNLFA scores representing material deprivation and household threat exposure.
This is because MNLFA scores are adjusted for these covariates.
Thus, it is recommended that variation in these covariates is also adjusted for in dependent variables [@bauer_2017].

We will calculate income-to-needs ratios by first taking the average of each binned income (\< \$5000, \$5,000--\$11,999, \$12,000--\$15,999, \$16,000--\$24,999, \$25,000--\$34,999, \$35,000--\$49,999, \$50,000--\$74,999, \$75,000--\$99,999, \$100,000--\$199,999, \≥ \$200,000) as a rough approximation of the family's total reported income.
Then we will divide income by the federal poverty threshold for the year at which a family was interviewed (range = \$12,486--\$50,681), adjusted for the number of persons in the home.
We will use highest education (in years) out of the two caregivers (or one if a second caregiver was not provided) as a continuous variable.
We will collapse youth race into 4 levels (White, Black, Hispanic, Other) and subsequently dummy-code with White (the most numerous racial group) serving as the reference category in all models.
We will dichotomize youth sex such that 1 = Female and 0 = Male.
We will use youth age (in months) as a continuous variable and centered on the mean.

## Proposed Analysis Pipeline {#meth_analyses}

### Planned main analyses {#meth_proposed}

Before conducting analyses, we will split the full sample up in a training set (*n* = 1,500) and a test set (*n* \≈ 8,500).
We will conduct our main analyses in three steps (each discussed in more detail below): (1) fit the DDM to the cognitive task data; (2) fit the SEM model to the adversity and DDM data and optimize it where necessary based on the training set; (3) Refit the model to the test data and interpret the regression coefficients.
We conducted a simulation-based power analysis based on the main SEM model (see Figure 3), with standardized regression coefficients of 0.06, 0.08 and 0.1 and the alpha level set to .05.
The analysis indicated that we will have more than 90% power for all regression paths with *N* between 2,500 ($\beta$ = 0.1) and 6,500 ($\beta$ = 0.06).

All analyses will be conducted in R 4.2.1 [@Rcoreteam_2022].
The source code can be found on the Github repository ([https://anonymous.4open.science/r/anon-255D/scripts](https://anonymous.4open.science/r/anon-255D/scripts)).

```{r figure2, fig.width=5, dpi=600, fig.id = "figure2", fig.cap.style = "Image Caption", fig.cap='**Figure 2.** Visual overview of the full analysis workflow. Analyses are done in two stages: (1) prior to Stage 1 submission of the manuscript, and (2) after Stage 1 in-principle acceptance. Analyses at Stage 1 only focus on the cognitive task data. Independent variables (i.e., threat and deprivation measures) will only be accessed during Stage 2 after all DDM models have been fit, and only for the test set after the model has been optimized based on the training set. Data access will be tracked via the GitHub repository. IV = independent variable; SEM = structural equation modeling; DDM = Drift Diffusion Model.'}
knitr::include_graphics("images/fig2.png")
```

<br>

**Step 1: DDM estimation.** The DDM will be fit to each cognitive task in a hierarchical Bayesian framework which estimates DDM parameters both on the individual and group level [@vandekerckhove_2011; @wiecki_2013].
We use code provided by @johnson_2017.
The benefit of this approach is that group-level information is leveraged to estimate individual-level estimates.
This differs from classic DDM estimation approaches where the model is fitted to the data of each participant separately [@voss_2013].
This is particularly useful in developmental samples like the ABCD dataset which have a limited number of trials per participant but substantially larger sample sizes than is typical in the DDM literature^2^.

All models will freely estimate the drift rate, non-decision time, and boundary separation while constraining response bias to 0.5 (i.e., assuming no bias towards a particular response option).
For the Flanker and Attention Shifting Task, we will compare model versions that separately estimate drift rate and non-decision-time per task condition or collapsed across conditions.
Boundary separation will be constrained to be the same across conditions.
For the Processing Speed Task and the Mental Rotation Task, we estimate DDM parameters across all trials.
The best-fitting model of each task will be used to estimate participant-level DDM parameters.
See the supplement for more information about model fitting procedures.

**Step 2: Model optimization in training set.** We will first estimate and (where necessary) optimize the SEM in the training set using the *lavaan* package [@rosseel_2012].
This goal of this step is to investigate whether we need to adjust the model specification in any way (e.g., add residual correlations, introduce or reduce constraints of factor loadings, etc.) to achieve good model fit.
For this reason, the model fitted in this step will not be interpreted to address our research aims.

See Figure 3 for the *a-priori* specification of the model.
In the measurement model, all three DDM parameters across all tasks (i.e., drift rates, non-decision times, and boundary separations) will load on separate latent factors for each parameter type.
Unique (residual) variances of the manifest (i.e., measured) DDM parameters will be captured in additional latent factors (one per parameter).
Age will be added as a covariate to each manifest DDM parameter.
The structural model will estimate regression paths going from each adversity measure (see [Adversity measures](#meth_adversity)) to the general latent factors and to the unique variances of the DDM parameters of each task.
For model identification reasons, we will not estimate regression paths to the unique variances of the Processing Speed Task.
We will first estimate and optimize the measurement models separately for each diffusion model parameter, which will allow us to efficiently detect sources of potential badness of fit.
Once measurement models provide an adequate account of the data, we will integrate them into the structural model shown in Figure 3.
In addition, clustering of siblings and twins within families will be accounted for using the *lavaan.survey* package [@oberski_2014].
Finally, the sociodemographic covariates that are included in the MNLFA scores (see Measures section above) will be controlled for in the SEM. 
Goodness-of-fit will be assessed using the root mean square error of approximation (RMSEA) and the comparative fit index (CFI).
Following @hu_1999, CFI values \> .90 and RMSEA values \< .08 will be interpreted as acceptable model fit and CFI values \> .95 and RMSEA values \≤ .06 as good model fit.

```{r figure3, fig.width=6, dpi=600, fig.id = "figure3", fig.cap.style = "Image Caption", fig.cap='**Figure 3.** Visualization of the full structural equation model (SEM). Dotted black lines represent covariances. Dashed black lines represent factor loadings. Solid grey lines represent regression paths. The factor loadings to each of the Processing Speed Task indicators are fixed to 1. The unique variances of each manifest indicator are captured in additional latent factors (U), one per indicator. To this end, the factor loadings are fixed to 0 and the residual variances of the manifest indicators are fixed to 0. For model identification reasons, we do not estimate regression paths to the unique variances of the Processing Speed Task. Not shown in this Figure to improve readability: (1) the sociodemographic covariates that are included in the MNLFA scores (see Measures section), and (2) age as a covariate for the manifest DDM parameters. PS = Processing Speed Task; AS Rep = Attention Shifting Task repeat trials; AS Sw = Attention Shifting Task switch trials; MR = Mental Rotation Task; FL Con = Flanker Task congruent trials; FL Inc = Flanker Task incongruent trials; *v* = Drift rate; *a* = Boundary separation; *t0* = Non-decision time; U = unique variance.'}
knitr::include_graphics("images/fig3.png")
```

<br>

**Step 3: Model validation in test set.** After optimizing the model based on the training set, we will refit it to the test data.
Model fit will be assessed the same way as at Step 2.
The regression coefficients of these models will be interpreted to address our research questions.
We will control for multiple testing in the regression paths based on the false discovery rate [@benjamini_1995; @cribbie_2007].
We will do so separately for tests involving drift rates, non-decision times, and boundary separations, as we have different hypotheses for each of these parameters.

## Timeline

All data required for the study have already been collected.
We estimate we will need three months after Stage 1 in-principle acceptance to fit all the DDM models, run the analyses, and finish the Stage 2 report.

\pagebreak

# Footnotes

^1^ The preregistration also included the Picture Vocabulary Task.
However, after accessing the data we realized that this task was implemented using computerized adaptive testing [@luciana_2018].
This makes it unsuitable for DDM, as the model assumes the level of difficulty is the same across trials.

^2^ We ran parameter recovery studies simulating the data for the Flanker Task, which has the lowest overall number of trials. Parameter recovery was excellent for the scenario that we plan in our main analyses (all *r*s \≥ .84). See the supplemental materials for more details.

\pagebreak

# References {#refs}

 

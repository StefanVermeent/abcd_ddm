---
bibliography: references.bib
csl: apa.csl
format: 
  docx:
    reference-doc: reference-doc.docx
output:
  officedown::rdocx_document:
    page_margins:
      bottom: 1
      footer: 0
      gutter: 0
      header: 0.5
      left: 1
      right: 1
      top: 1
    plots:
      align: center
      caption:
        pre: 'Figure '
        sep: '. '
        style: Image Caption
    tables:
      caption:
        pre: 'Table '
        sep: '. '
        style: Table Caption
  pdf_document: default
  word_document: default
editor: 
  markdown: 
    wrap: sentence
---

```{r include = FALSE}
library(flextable)
library(stringr)
library(dplyr)
library(tidyr)
library(tibble)
library(english)
library(lavaan)

source("../scripts/custom_functions/general-functions.R")
load("staged_results.RData")


knitr::opts_chunk$set(
  echo = F,
  fig.align = "center",
  fig.pos = "!t", 
  out.extra = "",
  fig.show = "asis",
  message = FALSE,
  tab.topcaption = T,
  warning = FALSE
)

# set up flextable for tables
set_flextable_defaults(
  font.family = "Times", 
  font.size = 10,
  font.color = "black",
  line_spacing = 1,
  padding.bottom = 1, 
  padding.top = 1,
  padding.left = 1,
  padding.right = 1
)

```

#### **Cognitive deficits and enhancements in youth from adverse conditions: An integrative assessment using Drift Diffusion Modeling in the ABCD study**

<br>

#### Stefan Vermeent^1,2^, Ethan S. Young^1^, Meriah L. DeJoseph^3^, Anna-Lena Schubert^4^, & Willem E. Frankenhuis^1,2,5^

#### ^1^ Department of Psychology, Utrecht University, Utrecht, The Netherlands

#### ^2^ Max Planck Institute for the Study of Crime, Security, and Law, Freiburg, Germany

#### ^3^ Institute of Child Development, University of Minnesota, USA

#### ^4^ Department of Psychology, University of Mainz, Mainz, Germany

#### ^5^ Evolutionary and Population Biology, Institute for Biodiversity and Ecosystem Dynamics, University of Amsterdam, Amsterdam, the Netherlands

<br>

# Data Availability

All scripts and materials needed to reproduce the findings are available on the article's Github repository [(https://anonymous.4open.science/r/anon-255D/README.md)](https://anonymous.4open.science/r/anon-255D/README.md).
We also include instructions on how to reproduce each step of our analyses.
To ensure computational reproducibility, we provide synthetic (i.e., simulated) data files with the same characteristics as the raw data.
The raw data supporting the findings of this study will be made available upon after Stage 2 via [https://doi.org/10.15154/1528297](https://doi.org/10.15154/1528297).

Data used in the preparation of this article were obtained from the Adolescent Brain Cognitive Development^SM^ (ABCD) Study (https://abcdstudy.org), held in the NIMH Data Archive (NDA). 
This is a multisite, longitudinal study designed to recruit more than 10,000 children age 9-10 and follow them over 10 years into early adulthood. 
The ABCD Study® is supported by the National Institutes of Health and additional federal partners under award numbers U01DA041048, U01DA050989, U01DA051016, U01DA041022, U01DA051018, U01DA051037, U01DA050987, U01DA041174, U01DA041106, U01DA041117, U01DA041028, U01DA041134, U01DA050988, U01DA051039, U01DA041156, U01DA041025, U01DA041120, U01DA051038, U01DA041148, U01DA041093, U01DA041089, U24DA041123, U24DA041147. 
A full list of supporters is available at [https://abcdstudy.org/federal-partners.html](https://abcdstudy.org/federal-partners.html). 
A listing of participating sites and a complete listing of the study investigators can be found at [https://abcdstudy.org/consortium_members/](https://abcdstudy.org/consortium_members/). 
ABCD consortium investigators designed and implemented the study and/or provided data but did not necessarily participate in the analysis or writing of this report. 
This manuscript reflects the views of the authors and may not reflect the opinions or views of the NIH or ABCD consortium investigators.
The ABCD data repository grows and changes over time. 
The ABCD data used in this report came from Data Release 4.0 (DOI: [http://dx.doi.org/10.15154/1523041](http://dx.doi.org/10.15154/1523041)).

# Funding Statement
WEF’s contributions have been supported by the Dutch Research Council (V1.Vidi.195.130) and the James S. McDonnell Foundation (https://doi.org/10.37717/220020502). 


# Disclosures
We declare no conflicts of interest.

# Ethics Approval Statement

This study was approved by the Ethics Review Board of the Faculty of Social & Behavioural Sciences of Utrecht University (FETC20-490).

\pagebreak

# Proposal Research Highlights

1. We use Drift Diffusion Modeling (DDM) to investigate how two forms of early-life adversity---material deprivation and household threat---are associated with lowered or improved cognitive performance.
2. The DDM can inform us if and where cognitive differences occur along distinct stages of cognitive processing.
3. We will also use structural equation modeling and out-of-sample validation to tease apart effects of adversity that are task-specific versus task-general.
4. We will apply our approach to a large, representative sample of around 10,000 9- to 10 year-olds from the ABCD study.

\pagebreak

# Proposal Abstract

Childhood adversity can lead to cognitive deficits or enhancements, depending on many factors. 
Though progress has been made, two challenges prevent us from integrating and better understanding these findings. 
First, studies commonly use and interpret raw performance differences, such as mean response times or overall accuracy. 
However, raw scores conflate different stages of cognitive processing. 
Second, studies tend to either isolate or aggregate abilities, obscuring the degree to which individual differences reflect task-specific or task-general processes. 
We address these challenges using Drift Diffusion Modeling (DDM) and structural equation modeling.
Combining these techniques, we can (1) relate early-life adverse experiences to individual differences in different stages of processing, and (2) investigate whether these reflect differences in specific or task-general performance. 
We examine how two forms of adversity---material deprivation and household threat---affect performance on four cognitive tasks in a large, representative sample of 9-10 year-olds from the Adolescent Brain Cognitive Development (ABCD) study. 
This approach holds promise for both deficit- and strength-based research questions. 
It will add much-needed nuance to adversity-related performance differences, which can inform theory and intervention.

*Keywords:* adversity, cognitive deficits, cognitive enhancements, drift diffusion modeling, Adolescent Brain Cognitive Development (ABCD) Study 

\pagebreak

#### Cognitive deficits and enhancements in youth from adverse conditions: An integrative assessment using Drift Diffusion Modeling in the ABCD study

<br>

The effects of early-life adversity---such as growing up in poverty or experiencing high levels of violence---on cognition are complex.
There is a growing consensus that adversity-exposed youth may develop not only deficits, but also strengths.
For example, studies find lowered and improved performance across different cognitive domains including (but not limited to) executive functioning, social cognition, language, and emotion [@ellis_2022; @frankenhuis_2013; @frankenhuis_2016;  @sheridan_2022; @sheridan_2014].
Researchers focused on one type of effect or another acknowledge the importance of identifying both deficits and strengths.
Yet, in practice, they often focus on one at the expense of the other.
To develop an integrated, well-rounded, and nuanced understanding of how adversity shapes cognitive abilities, research must integrate both types of effects.

Such an integration of deficit- and strength-based approaches is hampered by two methodological challenges.
First, most cognitive tasks involve different stages of processing which are obscured when analyzing raw performance differences.
This makes it difficult to understand why cognitive performance may be lowered or improved. 
Second, adversity may lower or improve performance because it affects general processes (i.e., processes shared across many tasks) or abilities that are task-specific.
In this Registered Report, we use a framework that tackles both challenges.
First, we decompose raw performance into measures of different stages of cognitive processes through cognitive modeling.
Second, we analyze four different tasks---tapping processing speed, attention shifting, inhibition, and mental rotation---all of which have documented associations with adversity.
Finally, we model shared (i.e., task-general) and unique (i.e., task-specific) factors that drive performance and investigate how they are associated with adversity.

## What do deficit and enhancement patterns mean? {#intro_sub1}

Both the deficit and strength-based literature often use speeded tasks, in which participants are usually instructed to respond as fast and accurate as possible.
For example, performing well on inhibition tasks [e.g., Flanker task, Go/No-Go Task\; @farah_2006; @fields_2021; @mezzacappa_2004; @noble_2005], attention shifting tasks [e.g., Dimensional Change Card Sort\; @farah_2006; @fields_2021; @nweze_2021; @young_2022; @mittal_2015; @noble_2005], and stimulus detection tasks [@farah_2006; @noble_2005; @pollak_2008] requires fast and accurate responses.
In practice, performance is often quantified using aggregated indices of speed alone (e.g., RT), accuracy alone (e.g., proportion correct), or both independently (rather than in an integrated manner).

In both the deficit and strength-based literature, *task performance* (indexed by mean RTs or accuracy) is routinely equated with *cognitive ability*.
For example, deficit-focused studies relate slower RTs on inhibition tasks to *worse inhibition ability* [@farah_2006; @fields_2021; @mezzacappa_2004; @noble_2005].
Strength-based studies relate faster RTs on standard attention shifting tasks to *better shifting ability* [@fields_2021; @young_2022; @mittal_2015].
However, speed and accuracy comprise more than pure ability (e.g., inhibition, attention shifting).
They also measure other constructs such as response caution (e.g., more or less cautious responding), speed of task preparation (e.g., orienting attention, encoding information), and speed of response execution. 
This heterogeneity creates an inferential risk, namely, if performance differences are interpreted as differences in abilities without sufficiently considering alternative explanations.
In addition, the effect of adversity exposure may not be limited to a single process.
For example, a specific type of adversity could affect both the speed of information processing and also shape the strategy that a person uses.
These inferential challenges have real-world implications, especially when raw performance is used as an early screening tool to assess cognitive abilities [@distefano_2021].

One promising solution to these issues is leveraging cognitive measurement models developed by mathematical psychologists.
For speeded binary decision tasks, a well-established measurement model is the Drift Diffusion Model [DDM\; @forstmann_2016; @ratcliff_1998; @ratcliff_2008; @wagenmakers_2009].
The DDM integrates speed and accuracy on a trial-by-trial level to estimate cognitive processes at different stages of the decision-making process.
The DDM assumes that people go through three distinct phases on each trial (see Figure 1 for a visualization).
The first phase, *preparation*, includes processes such as focusing attention and visually encoding the stimulus.
In the second phase, *decision-making*, people gather evidence for both response options until the evidence sufficiently favors one option over the other (explained below) and the decision process terminates. 
The third phase, *execution*, involves preparing and executing the motor response corresponding to the choice.

<br>
<br>
[Figure 1 about here]
<br>
<br>

DDM estimates a set of parameters^1^ for each participant that represent each phase of the decision process [@voss_2004]. 
The *drift rate* (*v*) represents the speed of information uptake [@schmiedek_2007; @voss_2013]. 
People with a higher drift rate are faster and make fewer errors. 
The *non-decision time* (*t0*) includes initial preparatory processes (e.g., visually encoding the stimulus) and processes after the decision is made (e.g., pressing a button). 
All else being equal, longer non-decision times reflect slower information processing but without a cost nor benefit in accuracy.
The *boundary separation* (*a*) represents the distance between the two decision boundaries.
A larger boundary separation means more information is collected before making a decision.
Thus, boundary separation measures response caution.
In contrast to non-decision time, larger boundary separation leads to slower but more accurate responses, reflecting a speed-accuracy tradeoff. 

As mentioned earlier, adversity-related raw performance differences---both lowered and improved performance---are typically interpreted as differences in ability (e.g., inhibition, attention shifting).
If these interpretations are accurate, then drift rate would reflect these variations.
That is because improved ability would result in both decreased RTs and increased accuracy.
However, if performance differences arise through other factors---such as differences in response caution or response speed---they would be captured by parameters other than the drift rate.
Thus, disentangling the drift rate, non-decision time, and boundary separation enhances our understanding of how adversity-exposure is associated with performance.

## Are deficit and enhancement patterns task-specific or task-general? {#intro_sub2}

An important caveat to interpreting task performance on any task in isolation is that performance on most tasks relies both on shared cognitive processes and unique abilities.
For example, RTs on executive functioning tasks are substantially confounded with general processing efficiency [@frischkorn_2019; @lerche_2020; @loffler_2022]. 
Both task-specific abilities and task-general processes affect RTs and accuracy in similar ways and are thus likely confounded in drift rates.
Task-general effects create the illusion that many different abilities are affected by adversity when in fact only one more general process is affected. 
Consider research on cognitive deficits. 
Adversity exposure might disrupt general cognitive processes shared across many tasks, such as general processing speed, for example, because of its effects on brain regions that are involved across several cognitive abilities [@sheridan_2014]. 
If so, studies analyzing raw Flanker performance in isolation will find processing speed deficits but wrongly interpret this as an inhibition deficit. 
Such distinctions matter for both deficit- and strength-based approaches (e.g., does adversity impair broad domains such as executive functioning? Does it enhance specific abilities such as attention shifting?), as well as for real-world interventions grounded in both approaches (e.g., school-based interventions targeting broad domains or specific abilities).

Structural equation modeling (SEM) can disentangle task-general and task-specific processes. 
For example, it can estimate shared task variance with latent task-general variables.
By estimating shared variance across different tasks, we can also obtain more precise estimates of task-specific abilities (i.e., variance unique to specific tasks).
@bignardi_2022 recently applied this approach to model how SES is related to standard performance measures in three large data sets.
They used SEM to model the effect of SES on a general factor and task-specific residual variances.
Lower SES was associated with a lower general ability, but *enhanced* task-specific processing speed, inhibition, and attention shifting.
However, their analysis looked at shared and unique variance using raw performance measures.
Thus, it is subject to the same limitations outlined in the previous section.

## The current study {#intro_current}

Here, we analyzed the Adolescent Brain Cognitive Development (ABCD) study data (http://abcdstudy.org). 
The ABCD study is ideal because it provides a large, representative, and socioeconomically and ethnically diverse sample of 9- to 10 year-olds--—an age range characterized by rapid growth in cognitive abilities [@blakemore_2006]. 

We studied two dimensions of adversity: household threat and material deprivation.
These forms of adversity have been widely studied in their relation to cognitive outcomes---from both deficit and strength-based perspectives [@fields_2021; @schafer_2022; @sheridan_2022; @young_2022]---and are central to contemporary conceptualizations of adversity [e.g., @mclaughlin_2021; @sheridan_2014]. 
Prior work has shown that cognitive deprivation is more strongly associated with lower cognitive performance than threat exposure [@salhi_2021; @sheridan_2020]. 
Although material deprivation (as measured here) and cognitive deprivation (in previous studies) are not identical, both seem related to access to resources that support cognitive development (e.g., books in the home, formal education). 
Indeed, in the ABCD sample material deprivation is highly or moderately correlated with income (-.81) and education (-.56), while correlations with household threat are lower [-.25 and -.12, respectively\; @dejoseph_2022]. 
Therefore, to the extent that the deprivation-versus-threat literature has captured ability-relevant processes, we may expect material deprivation to be more strongly associated with lower drift rates than threat exposure.

We analyzed four cognitive abilities that have been studied in relation to adversity. 
We included *attention shifting* because previous work has reported enhancement of this ability in children and (young) adults with more exposure to environmental unpredictability [based on raw performance switch costs\; @fields_2021; @mittal_2015; @young_2022; but see @nweze_2021]. 
Theoretically, attention shifting is thought to enable people to rapidly adjust to, and take advantage of, a changing environment (e.g., seize fleeting opportunities). 
We included *inhibition* because previous research suggests that children with more adverse experiences are worse at inhibiting distracting information [based on raw RT difference scores\; @fields_2021; @mezzacappa_2004; @mittal_2015; @tibu_2016].
We included *mental rotation* because previous studies have found negative associations between SES and mental rotation ability [based on RTs and accuracy\; @assari_2020; @bignardi_2022]. 
To the extent that these performance differences reflect differences in the respective abilities---as they have been interpreted---they should show up in *task-specific drift rates*.
We also included a measure of *processing speed*, which was not measured in relation to adversity but provided a direct measure of the type of basic processing speed that plays a role in the other tasks. 
Taken together, the four tasks provideda broad assessment of cognitive domains, which makes them well-suited for isolating task-general processes. 
As all four tasks adhere to DDM assumptions, we could compare them based on the same model parameters.

Adaptation-based frameworks predict increased task-specific drift rates.
This follows from the key assumption that adversity shapes specific abilities, rather than general cognitive processes [@ellis_2022; @frankenhuis_2020; @frankenhuis_2016; @frankenhuis_2013].
Task-specific enhancement in the attention-shifting drift rate would align with this assumption, as this ability is thought to be adaptive in changing environments; but enhancement in the task-general drift rate would not.
One study reports evidence suggesting that exposure to threat but not deprivation is associated with better attention shifting [@young_2022]. 
If so, we should expect to see higher task-specific drift rates with household threat, but not with material deprivation.
Enhanced task-specific drift rates on inhibition and mental rotation would be unexpected yet interesting.
It would constitute novel documentation of enhancements, and would suggest that lowered raw performance reflects ability-irrelevant processes.
Finally, equivalent drift rates across adversity levels would also not be consistent with strength-based frameworks; rather, such a pattern would suggest that abilities are intact (i.e., not affected by adversity).

Deficit perspectives can accommodate both lowered task-specific and lowered task-general drift rates.
On the one hand, past work suggests that adversity impairs specific abilities [e.g., inhibition\; @farah_2006; @fields_2021; @mezzacappa_2004; @mittal_2015]. 
On the other hand, there is also evidence that adversity affects general cognitive ability [@bignardi_2022]---perhaps through its broad effects on brain regions that are involved across several cognitive abilities [@sheridan_2014]. 
However, equivalent or enhanced drift rates, whether they be task-specific or task-general, would not be consistent with deficit perspectives; rather, this would suggest that abilities are intact or enhanced.

Our approach adds value in a third way besides separating drift rate from ability-irrelevant factors and isolating task-specific and task-general effects: It allows us to quantify cognitive deficits and enhancements separately within the same model.
This is because the task-specific and task-general estimates are statistically independent.
Thus, for instance, we may find that adversity lowers general drift rate, as well as some task-specific drift rate (e.g., capturing inhibition), but increases other task-specific drift rates (e.g., capturing attention shifting).

If the drift rates we observe align with previous interpretations of performance differences as outlined above, our findings support existing theories about deficits and enhancements. 
However, if not drift rates, but non-decision time or boundary separation account for the existing findings, and drift rates do not, neither deficit- or adaptation-based frameworks are supported. 
This would at a minimum invite reflection---perhaps revision---of the evidence base for (parts of) these frameworks.
At the same time, such findings would offer clear directions for future research in this field (e.g., which factors explain variation in non-decision times and/or boundary separation across levels of adversity).
Thus, regardless of the specific pattern of outcomes, our analyses contribute to an accurate and refined understanding of how early-life adversity shapes cognitive abilities.

# Methods {#methods}

## Sample {#meth_sample}

The ABCD study ([http://abcdstudy.org](http://abcdstudy.org)), is a prospective, longitudinal study of approximately 12,000 youth across the United States.
We focused on the baseline assessment, which has the largest collection of cognitive tasks suitable for DDM [@luciana_2018]. 
There were four tasks: (1) Processing Speed Task (Pattern Comparison Processing Speed Task), (2) Attention Shifting Task (Dimensional Change Card Sort Task), (3) Inhibition Task (Flanker Task), and (4) Mental Rotation Task (Little Man Task).
At baseline, the study included 11,878 youths (aged between 9 and 10 years old, measured in months) recruited across 21 sites. 
The study used multi-stage probability sampling to obtain a nationally representative sample [@heeringa_2010]. 
Baseline assessments were completed between September 1^st^ 2016 and August 31^st^ 2018 [see @garavan_2018].
Our analysis sample includes `r descriptives$precleaning_n$full_n_trial` participants who had trial-level data available on all four^2^ cognitive tasks.
*We will provide descriptive statistics of the youth included in the final sample (i.e., age, income-to-needs ratio, parent education level, ethnicity) in Stage 2 of this Registered Reports submission.*

## Open Science Statement {#meth_os}

All analysis scripts, materials, and instructions needed to reproduce the findings are available on the article's Github repository [(https://anonymous.4open.science/r/anon-255D/README.md)](https://anonymous.4open.science/r/anon-255D/README.md). 
The raw study data cannot be shared on public repositories.
Personal access to the ABCD dataset is required to fully reproduce our analyses and can be requested at [https://nda.nih.gov](https://nda.nih.gov).

We obtained access to the full ABCD data repository and performed initial data cleaning and analyses *prior* to Stage 1 submission.
However, we preprocessed cognitive task data in isolation to prevent biasing the analyses involving independent variables.
The goal of these analyses was to assure that the pre-selected cognitive tasks adhered to basic DDM assumptions and had the required trial-level data available in the right format.
These initial analyses were preregistered ([https://anonymous.4open.science/r/anon-255D/preregistrations/2022-09-20_preregistration_DDM.md](https://anonymous.4open.science/r/anon-255D/preregistrations/2022-09-20_preregistration_DDM.md)).

To increase transparency, we developed an automated workflow (using R and Git) to track the data files read into the analysis environment.
First-time access to any data file was automatically tracked via Git, providing an overview including the timestamp, a description of the data, and the R code that was used to read in the data.
The supplemental materials provide a detailed description and visual overview of this workflow.
An overview of the data access history is provided in the repository's README file ([https://anonymous.4open.science/r/anon-255D/README.md](https://anonymous.4open.science/r/anon-255D/README.md)).

## Exclusion Criteria {#meth_exclusions}

For the cognitive task data, we applied exclusion criteria in two steps: first, cleaning trial-level data, and second, removing participants with problematic trial-level data (discussed below).
For both, most criteria were as preregistered, but a few deviated from or were additional to the preregistration.
The data processing steps described below were preregistered unless noted otherwise.

First, we removed RTs of the Attention Shifting, Flanker, and Mental Rotation Tasks that exceeded maximum task-specific RT thresholds (\> 10 seconds (`r exclusions$dccs_trial$ex_RT_above_10`%), \> 10 seconds (`r exclusions$flanker_trial$ex_RT_above_10`%), and  \> 5 seconds (\< 0.01% of trials), respectively).
The Processing Speed Task did not have a programmed time-out.
Instead, we cut-off responses \> 10 seconds (`r exclusions$pcps_trial$ex_RT_above_10`% of trials) to remove extreme outliers.
This step was not preregistered as we did not anticipate these extreme outliers.

Next, we removed trials with: (1) RTs \< 300 ms (ranging from `r min(as.numeric(c(exclusions$lmt_trial$ex_fast_RT,exclusions$flanker_trial$ex_fast_RT,exclusions$dccs_trial$ex_fast_RT,exclusions$pcps_trial$ex_fast_RT,exclusions$picvoc_trial$ex_fast_RT)))`% to `r max(as.numeric(c(exclusions$lmt_trial$ex_fast_RT,exclusions$flanker_trial$ex_fast_RT,exclusions$dccs_trial$ex_fast_RT,exclusions$pcps_trial$ex_fast_RT,exclusions$picvoc_trial$ex_fast_RT)))`% of trials across tasks); (2) RTs \> 3 *SD* above the participant-level average log-transformed mean RT (ranging from `r min(as.numeric(c(exclusions$lmt_trial$ex_slow_RT,exclusions$flanker_trial$ex_slow_RT,exclusions$dccs_trial$ex_slow_RT,exclusions$pcps_trial$ex_slow_RT,exclusions$picvoc_trial$ex_slow_RT)))`% to `r max(as.numeric(c(exclusions$lmt_trial$ex_slow_RT,exclusions$flanker_trial$ex_slow_RT,exclusions$dccs_trial$ex_slow_RT,exclusions$pcps_trial$ex_slow_RT,exclusions$picvoc_trial$ex_slow_RT)))`% of trials across tasks; the same thing was done for RTs < 3 SD on the Processing Speed Task (not preregistered) to remove several fast outliers); (3) trials with missing response times and/or accuracy data (\< 0.01% for all tasks except Mental Rotation). We found that the response time-out of 5 seconds on the Mental Rotation Task led to missing responses on `r exclusions$lmt_trial$ex_missing_response`% of trials. 
This truncated the right-hand tail of the RT distribution, which can bias DDM estimation. 
Therefore, we decided to impute these values during DDM estimation instead of removing them (see the Supplemental materials for more information).

Next, we excluded participants who (1) had suffered possible mild traumatic brain injury or worse (*n* = `r smallNum(exclusions$lmt_participant$ex_tbi)`); (2) showed a response bias of \> 80% on a task (ranging between `r smallNum(min(as.numeric(c(exclusions$lmt_participant$ex_response_biases,exclusions$flanker_participant$ex_response_biases,exclusions$pcps_participant$ex_response_biases,exclusions$dccs_participant$ex_response_biases))))` and `r smallNum(max(as.numeric(c(exclusions$lmt_participant$ex_response_biases,exclusions$flanker_participant$ex_response_biases,exclusions$pcps_participant$ex_response_biases,exclusions$dccs_participant$ex_response_biases))))`; deviating from the preregistration); (3) had a low number of trials left after trial-level exclusions, defined as \< 20 trials for Mental Rotation and Attention Shifting (*n* = `r smallNum(exclusions$lmt_participant$ex_below_20_trials)` and `r smallNum(exclusions$dccs_participant$ex_below_20_trials)`, respectively) and \< 15 trials for Flanker and Processing Speed (*n* = `r exclusions$flanker_participant$ex_below_15_trials` and `r smallNum(exclusions$pcps_participant$ex_below_15_trials)`, respectively, deviating from the preregistration).
Finally, we excluded task data of several participants based on data inspection (not preregistered): `r smallNum(exclusions$lmt_participant$ex_0_accuracy)` participant with 0% accuracy on the Mental Rotation Task; `r smallNum(exclusions$pcps_participant$ex_decreasing_effort)` participants who showed a sharp decline in accuracy over time on the Processing Speed Task; `r smallNum(exclusions$dccs_participant$ex_switching_bias)` participants on the Attention Shifting Task who (almost) only made switches across all trials, even on repeat trials.
We also decided to include participants with missing data on one or more tasks because our main analyses will use FIML for missing data.

The final sample consisted of `r descriptives$postcleaning_n$full_n_trial` participants.

## Measures {#meth_measures}

### Cognitive Tasks {#meth_cogtasks}

**Flanker Task.** The NIH Toolbox Flanker task is a measure of cognitive control and attention [@zelazo_2014].
On each trial, participants saw five arrows that were positioned side-by-side.
The four flanking arrows always pointed in the same direction, either left or right.
The central arrow either pointed in the same direction (congruent trials) or in the opposite direction (incongruent trials).
Participants were instructed to always ignore the flanking arrows and to indicate whether the central arrow is pointing left or right.
After four practice trials, participants completed 20 test trials, of which 12 were congruent (*Mean~RT~* = `r descriptives$flanker$mean_rt_congruent` seconds, *SD* = `r descriptives$flanker$sd_rt_congruent`) and eight were incongruent (*Mean~RT~* = `r descriptives$flanker$mean_rt_incongruent` seconds, *SD* = `r descriptives$flanker$sd_rt_incongruent`).
The standard outcome measure is a normative composite of accuracy and RT. 
For more information on the exact calculation, see @slotkin_2012. 

**Processing Speed Task.** The NIH Toolbox Pattern Comparison Processing Speed task [@carlozzi_2015] is a measure of visual processing.
On each trial, participants saw two images and judged whether the images were the same or different.
When images were different, they varied on one of three dimensions: color, adding or taking something away, or containing more or less of a particular item.
The standard outcome measure is the number of items answered correctly in 90 seconds (normalized).
On average, participants completed `r pcps_clean |> count(subj_idx) |> summarise(n = mean(n)) |> summarise(mean = mean(n)) |> pull(mean) |> round(2)` trials (*Mean~RT~* = `r descriptives$pcps$mean_rt` seconds, *SD* = `r descriptives$pcps$sd_rt`).

**Attention Shifting Task.** The NIH Toolbox Dimensional Change Card Sort Task is a measure of attention shifting or cognitive flexibility [@zelazo_2006; @zelazo_2014].
A white rabbit and green boat were presented at the bottom of the screen.
Participants matched a third object to the rabbit or boat based on either color or shape.
After eight practice trials, participants completed 30 test trials alternating between shape and color in pseudo-random order.
Of these, 23 were *repeat* trials (i.e., the sorting rule was the same as on the previous trial; *Mean~RT~* = `r descriptives$dccs$mean_rt_repeat` seconds, *SD* = `r descriptives$dccs$sd_rt_repeat`) and 7 were *switch* trials (i.e., the sorting rule was different than on the previous trial; *Mean~RT~* = `r descriptives$dccs$mean_rt_switch` seconds, *SD* = `r descriptives$dccs$sd_rt_switch`).
The standard outcome measure is a normative composite of accuracy and RT.
For more information on the exact calculation, see @slotkin_2012. 

**Mental Rotation Task.** The Little Man task (referred to in this article as the Mental Rotation task) is a measure of visual-spatial processing [@luciana_2018].
Participants saw a simple picture of a male figure holding a briefcase in his left or right hand.
They had to indicate whether the briefcase was in the left or right hand.
The image could have one of four orientations: right side up or upside down, and facing towards or away from the participant.
Thus, on half of the trials, participants had to mentally rotate the image in order to make the decision.
Participants first completed three practice trials and then completed 32 test trials (*Mean~RT~* = `r descriptives$lmt$mean_rt`, *SD* = `r descriptives$lmt$sd_rt`).
The standard outcome measure is an efficiency measure, calculated as the percentage correct divided by the average RT.

### Adversity measures {#meth_adversity}

**Material deprivation.** We assessed material deprivation with seven items from the parent-reported ABCD Demographics Questionnaire.
These items originate from the Parent-Reported Financial Adversity Questionnaire [@diemer_2012].
The items assess whether or not (1 = Yes, 0 = No) the youth's family experienced several economic hardships over the 12 months prior to the assessment (e.g., 'Needed food but couldn't afford to buy it or couldn't afford to go out to get it').

We used a previously created factor score of this measure derived from MNLFA [@bauer_2017].
This score empirically adjusts for measurement non-invariance across sociodemographic characteristics and creates person-specific factor scores that enhance measurement precision and individual variation [@curran_2014].
In short, MNLFA scores assume a common scale of measurement across groups and age, as well as adjust for measurement biases that would have otherwise biased our substantive analyses.
@dejoseph_2022 describe how this score was computed.
Higher scores indicate more material deprivation.

**Household threat.** We assessed threat experienced in the youth's home using the Family Conflict subscale of the ABCD Family Environment Scale [@moos_1994; @zucker_2018].
The subscale consisted of nine items assessing conflict with family members (e.g., 'We fight a lot in our family').
Items were endorsed with either 1 (True) or 0 (False).
Two items are positively valenced and will therefore be reverse-scored.
Similar to material deprivation, we used a previously-created factor score of this measure derived from MNLFA [@dejoseph_2022].
Higher scores indicate more threat exposure.

**Sociodemographic covariates.** Several sociodemographic covariates were included in the SEM models (see [Planned Main Analyses](#meth_proposed)) that use the MNLFA scores representing material deprivation and household threat exposure.
This is because MNLFA scores are adjusted for these covariates.
Thus, it is recommended that variation in these covariates is also adjusted for in dependent variables [@bauer_2017].

We calculated income-to-needs ratios by first taking the average of each binned income (\< \$5000, \$5,000--\$11,999, \$12,000--\$15,999, \$16,000--\$24,999, \$25,000--\$34,999, \$35,000--\$49,999, \$50,000--\$74,999, \$75,000--\$99,999, \$100,000--\$199,999, \≥ \$200,000) as a rough approximation of the family's total reported income.
Then we divided income by the federal poverty threshold for the year at which a family was interviewed (range = \$12,486--\$50,681), adjusted for the number of persons in the home.
We used highest education (in years) out of the two caregivers (or one if a second caregiver was not provided) as a continuous variable.
We collapsed youth race into 4 levels (White, Black, Hispanic, Other) and subsequently dummy-coded with White (the most numerous racial group) serving as the reference category in all models.
We dichotomized youth sex such that 1 = Female and 0 = Male.
We used youth age (in months) as a continuous variable and centered on the mean.

## Proposed Analysis Pipeline {#meth_analyses}

### Planned main analyses {#meth_proposed}

Before conducting analyses, we split the full sample up in a training set (*n* = 1,500) and a test set (*n* \≈ 8,500).
We conducted our main analyses in three steps (each discussed in more detail below): (1) fitting the DDM to the cognitive task data; (2) fitting the SEM model to the adversity and DDM data and optimize it where necessary based on the training set; (3) Refitting the model to the test data and interpret the regression coefficients.
We conducted a simulation-based power analysis based on the main SEM model (see Figure 3), with standardized regression coefficients of 0.06, 0.08 and 0.1 and the alpha level set to .05.
The analysis indicated that we would have more than 90% power for all regression paths with *N* between 2,500 ($\beta$ = 0.1) and 6,500 ($\beta$ = 0.06).

All analyses were conducted in R 4.2.1 [@Rcoreteam_2022].
The source code can be found on the Github repository ([https://anonymous.4open.science/r/anon-255D/scripts](https://anonymous.4open.science/r/anon-255D/scripts)).

<br>
<br>
[Figure 2 about here]
<br>
<br>

**Step 1: DDM estimation.** The DDM was fit to each cognitive task in a hierarchical Bayesian framework which estimates DDM parameters both on the individual and group level [@vandekerckhove_2011; @wiecki_2013].
We use code provided by @johnson_2017.
The benefit of this approach is that group-level information is leveraged to estimate individual-level estimates.
This differs from classic DDM estimation approaches where the model is fitted to the data of each participant separately [@voss_2013].
This is particularly useful in developmental samples like the ABCD dataset which have a limited number of trials per participant but substantially larger sample sizes than is typical in the DDM literature^3^.

All models freely estimated the drift rate, non-decision time, and boundary separation while constraining response bias to 0.5 (i.e., assuming no bias towards a particular response option).
For the Flanker and Attention Shifting Task, we compared model versions that separately estimate drift rate and non-decision-time per task condition or collapsed across conditions.
Boundary separation was constrained to be the same across conditions.
For the Processing Speed Task and the Mental Rotation Task, we estimated DDM parameters across all trials.
The best-fitting model of each task was used to estimate participant-level DDM parameters.
See the supplement for more information about model fitting procedures.

**Step 2: Model optimization in training set.** We first estimated and (where necessary) optimized the SEM in the training set using the *lavaan* package [@rosseel_2012].
This goal of this step was to investigate whether we needed to adjust the model specification in any way (e.g., add residual correlations, introduce or reduce constraints of factor loadings, etc.) to achieve good model fit.
For this reason, the model fitted in this step was not interpreted to address our research aims.

See Figure 3 for the *a-priori* specification of the model.
In the measurement model, all three DDM parameters across all tasks (i.e., drift rates, non-decision times, and boundary separations) loaded on separate latent factors for each parameter type.
Unique (residual) variances of the manifest (i.e., measured) DDM parameters were captured in additional latent factors (one per parameter).
The structural model estimated regression paths going from each adversity measure (see [Adversity measures](#meth_adversity)) to the general latent factors and to the unique variances of the DDM parameters of each task.
For model identification reasons, we did not estimate regression paths to the unique variances of the Processing Speed Task.
We first estimated and optimized the measurement models separately for each diffusion model parameter, which allowed us to efficiently detect sources of potential badness of fit.
Once measurement models provided an adequate account of the data, we integrated them into the structural model shown in Figure 3.
In addition, clustering of siblings and twins within families was accounted for using the *lavaan.survey* package [@oberski_2014].
Finally, the sociodemographic covariates that were included in the MNLFA scores (see Measures section above) were controlled for in the SEM. 
Goodness-of-fit was assessed using the root mean square error of approximation (RMSEA) and the comparative fit index (CFI).
Following @hu_1999, CFI values \> .90 and RMSEA values \< .08 were interpreted as acceptable model fit and CFI values \> .95 and RMSEA values \≤ .06 as good model fit.

<br>
<br>
[Figure 3 about here]
<br>
<br>

**Step 3: Model validation in test set.** After optimizing the model based on the training set, we refit it to the test data.
Model fit was assessed the same way as at Step 2.
The regression coefficients of these models were interpreted to address our research questions.
We controlled for multiple testing in the regression paths based on the false discovery rate [@benjamini_1995; @cribbie_2007].
We did so separately for tests involving drift rates, non-decision times, and boundary separations, as we had different hypotheses for each of these parameters.
In addition, we were interested in determining if standardized effects that fell between -.10 and .10 were consistent with an actual null effect.
For regression coefficients falling within these bounds, we therefore used two one-sided tests (TOST) equivalence testing using -.10 and .10 as bounds.

# Results

## Model fit

### DDM

Based on a model fit assessment, we selected the following good-fitting DDM models for the substantive analysis: 1) *Mental Rotation Task*, the base model with imputed RTs; 2) *Flanker Task*, the model with one set of parameter estimates across conditions; 3) *Attention Shifting Task*, the model with one set of parameter estimates across conditions; 4) *Processing Speed Task*, the base model, but with RTs < 1 s excluded to solve issues with fast outliers.
See the supplemental materials for a full overview of the DDM fitting results.

Our model fit procedure deviated from the preregistered plan by simulating 100 trials instead of the same number of trials as the participants.
As the number of trials for each task was low, simulating the same number of trials led to a very low number of data points at each RT quantile.
While we increased the number of simulated trials, the real data still only had a low number of data points at each quantile.
The fact that correlations for both RTs and accuracy were high across all tasks (all ≥ .79, with many >.90) increased our confidence that the estimates were reliable, and that further changes were unnecessary.
The supplemental materials present model fit results for both the preregistered approach and the updated approach.

Table 1 shows bivariate correlations between DDM parameters and adversity measures.
Both material deprivation and household threat showed small, negative associations with drift rates across all four tasks, suggesting that participants with more adversity exposure processed information more slowly.
In addition, both material deprivation and household threat tended to be positively associated with boundary separation (indicating more response caution, except for Mental Rotation), although most correlations were very small.
Finally, material deprivation and household threat showed a small, negative correlation with non-decision times on the Mental Rotation Task, but not with non-decision times on the other tasks.


\pagebreak

```{r}
#| tab.id: table1
#| results: markup
table1
```

### SEM

The SEM model was incrementally constructed in the training data in order to detect any parts that might need adjustments. 
All parts of the model provided an acceptable to good account of the training data (full training model: CFI = `r training_sem_full_cluster |> fitmeasures() |> enframe() |> filter(name == 'cfi') |> pull(value) |> formatC(digits = 3, width = 3, flag = "0", format = 'f') |> str_remove(pattern = "^0")`, RMSEA = `r training_sem_full_cluster |> fitmeasures() |> enframe() |> filter(name == 'rmsea') |> pull(value) |> formatC(digits = 3, width = 3, flag = "0", format = 'f') |> str_remove(pattern = "^0")`).
Therefore, we did not make any adjustments to the model before applying it to the test data (N = 9063).
The full model also provided a good account of the test data (CFI = `r test_sem_full_cluster |> fitmeasures() |> enframe() |> filter(name == 'cfi') |> pull(value) |> formatC(digits = 3, width = 3, flag = "0", format = 'f') |> str_remove(pattern = "^0")`, RMSEA = `r test_sem_full_cluster |> fitmeasures() |> enframe() |> filter(name == 'rmsea') |> pull(value) |> formatC(digits = 3, width = 3, flag = "0", format = 'f') |> str_remove(pattern = "^0")`).

Figure 4 presents a simplified overview of the measurement part of the final model in the test data (excluding task-specific covariances and regression paths). 
The factor loadings of the Mental Rotation Task were low for all DDM parameters, suggesting that performance on this task differs substantially from performance on the other tasks. 
All tasks showed a statistically significant portion of task-specific variance after accounting for task-general effects. 
Task-general drift rate and task-general boundary separation were negatively correlated (*r* = `r test_sem_full_cluster |> parameterEstimates(standardized = T) |> filter(op == "~~", lhs == "v_general", rhs == "a_general") |> pull(std.all) |> formatC(digits = 2, width = 3, flag = "0", format = 'f') |> str_remove(pattern = "^0")`).
Task-general boundary separation and task-general non-decision time were positively correlated (*r* = `r test_sem_full_cluster |> parameterEstimates(standardized = T) |> filter(op == "~~", lhs == "a_general", rhs == "t_general") |> pull(std.all) |> formatC(digits = 2, width = 3, flag = "0", format = 'f') |> str_remove(pattern = "^0")`).
Task-specific correlations between DDM parameters of the same tasks ranged between *r* = `r test_sem_full_cluster |> parameterEstimates(standardized = T) |> filter(op == "~~", str_detect(lhs, "_l$"), str_detect(rhs, "_l$")) |> drop_na(z) |> filter(lhs != rhs) |> pull(std.all) |> abs() |> min() |> formatC(digits = 2, width = 3, flag = "0", format = 'f') |> str_remove(pattern = "^0")` and *r* = `r test_sem_full_cluster |> parameterEstimates(standardized = T) |> filter(op == "~~", str_detect(lhs, "_l$"), str_detect(rhs, "_l$")) |> drop_na(z) |> filter(lhs != rhs) |> pull(std.all) |> abs() |> max() |> formatC(digits = 2, width = 3, flag = "0", format = 'f') |> str_remove(pattern = "^0")`.

<br>

```{r}
#| label: Figure4
#| fig-width: 6.25
#| fig-height: 7
#| dpi: 600
#| out-width: 6in
#| fig-cap: |
#|   **Figure 4.** Simplified overview of the measurement part of the final SEM model, including standardized factor loadings, unstandardized residual variances, and correlations between the general latent factors. Excluding task-specific residual covariances and regression paths (see Figure 5). The latent factors at the top (the elipses) represent the task-general factors. The latent factors at the bottom (the circles) represent task-specific factors. *v* = drift rate; *a* = boundary separation; *t0* = non-decision time; PS = Processing Speed Task; AS = Attention Shifting Task; MR = Mental Rotation Task; Fl = Flanker Task.

knitr::include_graphics("images/fig4.png")
```

## Primary analysis

Our primary analysis examined to what extent household threat and material deprivation were associated with task-specific and task-general aspects of speed of information processing (drift rates), response caution (boundary-separations), and task preparation/execution (non-decision times).
Task-general effects capture variance that is shared across tasks, whereas task-specific effects capture variance that is unique to specific tasks.
The results are summarized in Figure 4.

```{r}
#| label: Figure5
#| fig-width: 6.25
#| fig-height: 7
#| dpi: 600
#| out-width: 6in
#| fig-cap: | 
#|   **Figure 4.** Results of the structural part of the SEM model testing the effect of household threat and material deprivation on task-specific and task-general DDM parameters. The top row plots the drift rates, the middle row plots the boundary separations, and the bottom row plots the non-decision times. The gray area reflects the area of practical equivalence. Solid points indicate effects that are outside the area of practical equivalence. Hollow points indicate effects that fall inside the area of practical equivalence. Standard-errors represent 95% confidence intervals. Statistical significance (tested against zero) is indicated with significance stars. \
#|   \*\*\* *p* < .001,  \*\* *p* < .01,  \* *p* < .05

fig5
```

For household threat, we found a significant, negative association with task-general drift rate ($\beta$ = `r test_reg_coef_list$v_general_threat_mnlfa$est.std`, 95% CI = [`r test_reg_coef_list$v_general_threat_mnlfa$ci.lower`, `r test_reg_coef_list$v_general_threat_mnlfa$ci.upper`], *p* `r test_reg_coef_list$v_general_threat_mnlfa$pvalue_adj_chr`), indicating that participants with more exposure to household threat processed information more slowly in general.
All task-specific drift rates were practically equivalent at different levels of household threat.
We also found a significant, positive association between household threat and task-general boundary separation ($\beta$ = `r test_reg_coef_list$a_general_threat_mnlfa$est.std`, 95% CI = [`r test_reg_coef_list$a_general_threat_mnlfa$ci.lower`, `r test_reg_coef_list$a_general_threat_mnlfa$ci.upper`], *p* `r test_reg_coef_list$a_general_threat_mnlfa$pvalue_adj_chr`), indicating that participants with more exposure to household threat generally responded with more caution.
In contrast, we found a negative association between household threat and task-specific boundary separation on the Attention Shifting Task ($\beta$ = `r test_reg_coef_list$dccs_a_l_threat_mnlfa$est.std`, 95% CI = [`r test_reg_coef_list$dccs_a_l_threat_mnlfa$ci.lower`, `r test_reg_coef_list$dccs_a_l_threat_mnlfa$ci.upper`], *p* = `r test_reg_coef_list$dccs_a_l_threat_mnlfa$pvalue_adj_chr`), indicating that participants with more exposure to household threat responded with less caution.
The association between household threat and task-specific boundary separation on the Flanker Task was also significant, but fell in the region of practical equivalence.
Both task-general non-decision time and task-specific non-decision times were practically equivalent at different levels of household threat.

For material deprivation, the associations with task-general drift rate, as well as with all task-specific drift rates, were not significantly different from zero.
We found evidence for practical equivalence for task-general drift rate and the task-specific drift rates of the Flanker Task and the Mental Rotation Task.
However, we did not find evidence for practical equivalence for the task-specific drift rate of Attention Shifting, suggesting that participants with higher levels of material deprivation might be somewhat slower at shifting attention.
the association between material deprivation and task-general boundary separation was not significantly different from zero ($\beta$ = `r test_reg_coef_list$a_general_dep_mnlfa$est.std`, 95% CI = [`r test_reg_coef_list$a_general_dep_mnlfa$ci.lower`, `r test_reg_coef_list$a_general_dep_mnlfa$ci.upper`], *p* = `r test_reg_coef_list$a_general_dep_mnlfa$pvalue_adj_chr`), although we also did not find evidence for practical significance. 
Thus, participants with more exposure to material deprivation might generally respond with somewhat more caution.
All of the task-specific boundary separations were practically equivalent at different levels of material deprivation.
Both task-general non-decision time and task-specific non-decision times were practically equivalent at different levels of material deprivation.

Taken together, we did not find task-specific enhanced abilities in line with adaptation-based frameworks.
Our finding of a negative association between household threat and task-general drift rate was in line with the deficit framework, as was the negative association between material deprivation and the task-specific drift rate of the Attention Shifting Task (although the evidence for this association was mixed).
However, all other task-specific drift rates were practically equivalent at different levels of adversity, which is not in line with either framework.
In addition, we found differences in boundary separation both on a task-general and task-specific level, which leads to performance differences that are unrelated to ability.

## Exploratory analysis

To contextualize our primary, preregistered findings, we decided to run additional exploratory models in the test data that analyzed the associations between adversity and traditional, raw performance measures of the four cognitive tasks.
We used the measures as provided in the ABCD database [@luciana_2018].
For the Processing Speed Task, the traditional raw measure is the number of correctly completed trials.
For the Mental Rotation Task, the traditional raw measure is the percentage correct divided by the mean response time on correct trials.
For the Attention Shifting and Flanker Task, the traditional raw measure is a composite of accuracy and RT [@slotkin_2012].
Per task, we ran a linear regression model including the traditional outcome measure as the dependent variable, and including as independent variables the MNLFA-corrected material deprivation and household threat measures, as well as the covariates used in calculating the MNLFA scores (age (in months, centered), sex (dummy-coded), ethnicity (dummy-coded, with "white" as the reference category), and highest parental education).

Household threat was significantly negatively associated with raw performance on the Processing Speed Task ($\beta$ = `r expl_results_list$threat_mnlfa_pcps$Std_Coefficient`, 95% CI = [`r expl_results_list$threat_mnlfa_pcps$CI_low`, `r expl_results_list$threat_mnlfa_pcps$CI_high`], *p* `r expl_results_list$threat_mnlfa_pcps$p.value`) and the Attention Shifting Task ($\beta$ = `r expl_results_list$threat_mnlfa_dccs$Std_Coefficient`, 95% CI = [`r expl_results_list$threat_mnlfa_dccs$CI_low`, `r expl_results_list$threat_mnlfa_pcps$CI_high`], *p* `r expl_results_list$threat_mnlfa_pcps$p.value`). Household threat was not associated with raw performance on the Flanker Task (*p* = `r expl_results_list$threat_mnlfa_flanker$p.value`) and Mental Rotation Task (*p* = `r expl_results_list$threat_mnlfa_lmt$p.value`).
Material deprivation was not significantly associated with any of the tasks (all *p*s > `r expl_results_list$dep_mnlfa_dccs$p.value`).

# Discussion

Our aim was to better understand how two types of adversity---household threat and material deprivation---are associated with lowered and improved performance on three tasks covering inhibition, attention shifting, and mental rotation.
First, we used DDM to distinguish between three potential sources for performance differences: 1) the speed of information processing (drift rates), 2) response caution (boundary separation), and 3) speed of task preparations and response execution (non-decision time).
Second, we used SEM to investigate if observed differences in each DDM parameter were task-general (i.e., shared across all tasks) or task-specific (i.e., unique to a specific task).
Negative associations between adversity and either task-general or task-specific drift rates would be consistent with existing deficit frameworks.
Positive associations between adversity and task-specific drift rates would be consistent with existing adaptation frameworks.
In contrast, associations with other DDM parameters, or equivalent drift rates, would not be consistent with either framework.

## Evidence for task-general deficits, intact task-specific performance, and ability-irrelevant differences

Our results provided some support for deficit frameworks, and did not provide support for adaptation frameworks.
Higher levels of household threat (but not material deprivation) were associated with lower task-general speed of information processing.
This was consistent with deficit frameworks, although based on previous literature, we expected stronger deficit patterns for deprivation than for threat [@salhi_2021; @sheridan_2014; @sheridan_2020].
Inconsistent with either deficit or adaptation frameworks, all but one of the task-specific abilities showed intact performance.
The only exception was the negative association between material deprivation and attention shifting, where we did not find evidence for a significant performance difference, nor for truly intact performance.
Finally, both household threat and material deprivation led to more response caution, although the evidence for material deprivation was weak (not significantly different from zero, but also not practically equivalent to zero).
We did not find any differences in task-general or task-specific aspects of task preparation and response execution.

The finding that most task-specific abilities were unaffected by either type of adversity was striking in light of the existing literature.
It suggests that the cognitive abilities of youths with more adversity exposure were with those of their counterparts from low-adversity contexts.
This is inconsistent with previous interpretations of adversity-related performance differences based on raw performance measures.
For example, previous study showed enhanced attention-shifting performance in youth with more exposure to threat [@young_2022\; see also @mittal_2015; @fields_2021 for similar findings with environmental and caregiver unpredictability].
In addition, youth from adversity have previously been found to perform worse on inhibition tasks [@farah_2006; @fields_2021; @mezzacappa_2004; @mittal_2015; @noble_2005], and previous investigations in the ABCD study found negative associations between SES and mental rotation [@assari_2020; @bignardi_2022].

Instead, higher levels of household threat were associated with task-general information processing.
This raises the question of which process (or processes) is captured by this factor.
We see two non-mutually exclusive possibilities.
First, several studies have uncovered a collection of brain regions that are activated during performance on a wide range of executive functioning tasks, but not during performance on other types of tasks [@cole_2013; @duncan_2000; @niendam_2012; @sheridan_2014].
These brain regions might be relevant for more general aspects of executive functioning, such as goal maintenance, attention-regulation, and monitoring [@zelazo_2023].
Second, executive functioning tasks are confounded with basic processing efficiency that is not unique to executive functioning [@frischkorn_2019; @lerche_2020], with one study suggesting that basic processing speed may be the predominant factor explaining individual differences on executive functioning tasks [@loffler_2022]. 

Both types of processes---general aspects of executive functioning and basic speed of processing---are possible candidates for the task-specific drift rate factor.
Aside from the three tasks measuring aspects of executive functioning, we included a measure of basic processing speed to use as an anchor for the task-general factors.
In addition, the drift rates of the Flanker Task and Attention Shifting task included variance of the relatively simple congruent and repeat trials, respectively.
Thus, at least to some degree, the task-general drift rate factor should reflect basic speed of information processing.
However, there are three reasons why it seems unlikely that our task-general drift rate factor merely reflects basic speed of processing.
First, although the Processing Speed Task is conceptually simple [@carlozzi_2015] and showed high overall accuracy, response times were longer than expected, suggesting that performance might not reflect pure processing speed.
Second, a significant portion of task-specific variance still remained in the Processing Speed Task after accounting for task-general variance, suggesting that individual differences on the task are not only related to basic speed of processing.
Third, the Flanker and Attention Shifting Task contributed more variance to the task-general drift rate factor than the Processing Speed Task, as was visible from their higher factor loadings. 

Our results also align to some extent with two recent investigations.
First, @bignardi_2022 conducted a study in three large datasets---among which the ABCD study---in which they used SEM to separate task-general variance from task-specific variance.
They found that SES was positively associated with lower task-general performance in all datasets, but after accounting for task-general performance found many instances of practically equivalent performance.
Interestingly, they found negative associations (meaning better performance) between SES and the Flanker and Attention Shifting Task in the ABCD data.
Second, @young_2023 investigated associations between socioeconomic harshness and unpredictability and performance on subtasks of the Woodcock-Johnson achievement task battery, comparing subtask performance to overall performance.
Socioeconomic harshness was associated with lower overall performance, which seemed strongly driven by vocabulary tasks.
However, most subtasks showed intact performance relative to overall performance, with auditory tasks even showing relative enhancements.

Household threat (and to a lesser extent material deprivation) was also associated with more response caution.
This is an important finding, as more response caution leads to longer response times.
Traditional assessments focusing only on response times could misinterpret higher response caution as an impaired ability.
In contrast, task-specific response caution was *lower* for the Attention Shifting and Flanker Task, although the latter was found to be practically equivalent.
This could indicate that youth with more household threat adjust their level of response caution, using more caution on one type of trials and less caution on the other type of trials. 
This would be an unexpected finding, as it is generally assumed that people cannot adjust their response caution if conditions are randomized across trials (which is also why we defaulted to fix boundary separations across conditions).

## Strengths, limitations, and future directions

The current study has several strengths. 
First, the analyses were based on the ABCD sample, a large, representative US sample.
Second, we developed a framework that can simultaneously account for adversity-related impairments and enhancements, and provides a more detailed understanding of the processes that are (un)affected by adversity.
Third, we used measures of material deprivation and household threat that were corrected for measurement non-invariance using MNLFA, resulting in unbiased estimates of both dimensions of adversity.
In addition to these strengths, the study also had limitations.
First, we only included a relatively small set of cognitive abilities that were compatible with DDM assumptions.
This inevitably excluded many important abilities, which limited the scope of what is captured both in task-general and task-specific processes.
Second, the measures of household threat and material deprivation were binary, asking for the presence or absence of certain exposures over the last 12 months.
Therefore, we were not able to determine how chronic these experiences were, and whether some youth experienced them more frequently than others.

Future research can build on this study in a couple of ways.
First, it would be important to better understand the processes making up task-general drift rate.
To this end, future research should include measures of candidate processes (e.g., basic processing speed, attention maintenance), ideally several measures per process to get good latent estimates.
In addition, neuro-imaging data could be linked directly to DDM parameters to investigate which brain networks are behind differences in task-general drift rates.
Second, future research could aim to better understand task-general and task-specific differences in response caution.
For example, do youth from adversity show more task-general response caution out of performance anxiety?
If so, are there task-specific differences in youths performance anxiety?
Third, and more generally, the framework developed here could be used to bridge achievement gaps in a more principled fashion.
For example, future studies could assess whether the use of more ecologically relevant stimuli or testing contexts improve information processing, and whether such interventions target task-general or task-specific aspects of performance.

We believe the approach could also add value in light of perspectives arguing for more culturally-sensitive assessments of executive functioning that relates better to youths pre-existing goals, values, and lived experiences [@doebel_2020; @miller_cotto_2020\; also see @zelazo_2023].
We agree with these perspectives, but emphasize that more ecologically relevant assessments---to the extent that they also rely on response times and accuracy---will still suffer the same methodological limitations as traditional tasks.
This is exemplified by recent attempts to make task-content more ecologically relevant.
While promising, the effects are sometimes difficult to interpret, with different types of content affecting performance in unexpected and inconsistent ways, and not always for the better [@duquennois_2022; @muskens_2019; @young_2022].
Our approach could offer a richer understanding of how different types of interventions and learning materials affect different stages of cognitive processing.

## Conclusion

Taken together, we find that adversity is mostly associated with task-general processes, as well as ability-irrelevant response caution, but that task-specific abilities are mostly intact.
This suggests that traditional cognitive assessments may overestimate the effect of adversity on specific abilities (both impairments and enhancements).
Our analytical approach provides a solution. 
By combining DDM and SEM approaches, we can start to develop a more nuanced understanding of how adversity affects different aspects of cognitive performance.
This approach requires large datasets containing multiple cognitive tasks, a requirement that is increasingly feasible with the availability of large, secondary datasets in developmental science [@kievit_2022].
With it, we can develop a more balanced, well-rounded understanding that integrate both deficit and adaptation perspectives.

\pagebreak

# Footnotes

^1^ A fourth DDM parameter, the *starting point* (*z*), represents an initial bias towards one of the two decision options (e.g., a tendency to classify facial expressions as angry that extends to neutral faces).
Note that allowing the starting point to vary only makes sense if response options differ in valence (e.g., happy and angry faces, which the current study does not include and thus is unable to examine).

^2^ The preregistration also included the Picture Vocabulary Task.
However, after accessing the data we realized that this task was implemented using computerized adaptive testing [@luciana_2018].
This makes it unsuitable for DDM, as the model assumes the level of difficulty is the same across trials.

^3^ We ran parameter recovery studies simulating the data for the Flanker Task, which has the lowest overall number of trials. Parameter recovery was excellent for the scenario that we plan in our main analyses (all *r*s \≥ .84). See the supplemental materials for more details.

\pagebreak

# References

::: {#refs}
:::

\pagebreak

```{r}
#| label: figure1
#| fig.width: 5.5
#| dpi: 600
#| fig-cap: | 
#|   **Figure 1.** A visual overview of the Drift Diffusion Model (DDM). The DDM assumes that decision making on cognitive tasks with two forced response options advances through three stages. First, people go through a preparation phase in which they engage in initial stimulus encoding. Second, people gather information for one of two response options until the accumulation process terminates at one of the decision boundaries. Each squiggly line  represents the evidence accumulation process on a single trial. Third, a motor response is triggered in the execution phase. The model estimates four parameters that reflect distinct cognitive processes (printed in italic): (1) The *drift rate* represents the rate at which evidence accumulation drifts towards the decision boundary and is a measure of processing speed; (2) The *non-decision time* represents the combined time spent on task preparation and response execution; (3) The *boundary separation* represents the width of the decision boundaries and is a measure of response caution; (4) The *starting point* represents the starting point of the decision process and can be used to model response biases (not considered in this study). 
knitr::include_graphics("images/fig1.png")
```

\pagebreak

```{r}
#| label: figure2
#| fig-width: 5
#| dpi: 600
#| fig-cap: | 
#|   **Figure 2.** Visual overview of the full analysis workflow. Analyses are done in two stages: (1) prior to Stage 1 submission of the manuscript, and (2) after Stage 1 in-principle acceptance. Analyses at Stage 1 only focus on the cognitive task data. Independent variables (i.e., threat and deprivation measures) will only be accessed during Stage 2 after all DDM models have been fit, and only for the test set after the model has been optimized based on the training set. Data access will be tracked via the GitHub repository. IV = independent variable; SEM = structural equation modeling; DDM = Drift Diffusion Model. 
knitr::include_graphics("images/fig2.png")
```

\pagebreak

```{r}
#| label: figure3
#| fig-width: 6
#| dpi: 600
#| fig-cap: | 
#|   **Figure 3.** Visualization of the full structural equation model (SEM). Dotted black lines represent covariances. Dashed black lines represent factor loadings. Solid grey lines represent regression paths. The factor loadings to each of the Processing Speed Task indicators are fixed to 1. The unique variances of each manifest indicator are captured in additional latent factors (U), one per indicator. To this end, the factor loadings are fixed to 1 and the residual variances of the manifest indicators are fixed to 0. For model identification reasons, we do not estimate regression paths to the unique variances of the Processing Speed Task. Not shown in this Figure to improve readability: (1) the sociodemographic covariates that are included in the MNLFA scores (see Measures section). PS = Processing Speed Task; AS Rep = Attention Shifting Task repeat trials; AS Sw = Attention Shifting Task switch trials; MR = Mental Rotation Task; FL Con = Flanker Task congruent trials; FL Inc = Flanker Task incongruent trials; *v* = Drift rate; *a* = Boundary separation; *t0* = Non-decision time; U = unique variance. 
knitr::include_graphics("images/fig3.png")
```


